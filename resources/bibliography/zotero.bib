
@article{Lecun1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	doi = {10.1109/5.726791},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y and Bottou, L and Bengio, Y and Haffner, P},
	year = {1998},
	keywords = {IEEE\_Xplore},
	pages = {2278--2324},
}

@misc{IndoML2018,
	title = {Student {Notes}: {Convolutional} {Neural} {Networks} ({CNN}) {Introduction} – {Belajar} {Pembelajaran} {Mesin} {Indonesia}},
	url = {https://indoml.com/2018/03/07/student-notes-convolutional-neural-networks-cnn-introduction/},
	author = {{IndoML}},
	month = feb,
	year = {2018},
}

@inproceedings{Choromanska2014,
	title = {The {Loss} {Surfaces} of {Multilayer} {Networks}},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	author = {Choromańska, Anna and Henaff, Mikael and Mathieu, Michaël and Arous, Gérard Ben and LeCun, Yann},
	year = {2014},
	note = {arXiv: 1412.0233v3},
	keywords = {Semantic\_Scholar},
}

@article{Rumelhart1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	doi = {10.1038/323533a0},
	journal = {Nature},
	author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
	year = {1986},
	keywords = {Semantic\_Scholar},
	pages = {533--536},
}

@article{Tsalera2021,
	title = {Comparison of {Pre}-{Trained} {CNNs} for {Audio} {Classification} {Using} {Transfer} {Learning}},
	volume = {10},
	issn = {2224-2708},
	url = {https://www.mdpi.com/2224-2708/10/4/72},
	doi = {10.3390/jsan10040072},
	number = {4},
	journal = {Journal of Sensor and Actuator Networks},
	author = {Tsalera, Eleni and Papadakis, Andreas and Samarakou, Maria},
	year = {2021},
	keywords = {Semantic\_Scholar},
}

@misc{Teslanorth2023,
	title = {Tesla {Model} {Y} {HW4} {Infotainment} {System}: {Lower} {Specs} vs {HW3} - {TeslaNorth}.com},
	url = {https://teslanorth.com/2023/08/07/tesla-model-y-hw4-infotainment-system/},
	author = {Tesla North},
	month = feb,
	year = {2009},
}

@inproceedings{Chachada2013,
	title = {Environmental sound recognition: {A} survey},
	doi = {10.1109/APSIPA.2013.6694338},
	booktitle = {2013 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference}},
	author = {Chachada, Sachin and Kuo, C.-C. Jay},
	year = {2013},
	keywords = {IEEE\_Xplore, Mel frequency cepstral coefficient;Feature extraction;Hidden Markov models;Speech;Accuracy;Atomic clocks},
	pages = {1--9},
}

@misc{Aptiv2020,
	title = {What {Is} an {Electronic} {Control} {Unit}?},
	url = {https://www.aptiv.com/en/insights/article/what-is-an-electronic-control-unit},
	author = {{APTIV}},
	month = feb,
	year = {2020},
}

@misc{Infineon2024,
	title = {Automotive {Head} {Unit} - {Infineon} {Technologies}},
	url = {https://www.infineon.com/cms/en/applications/automotive/infotainment/head-unit/#!products},
	author = {{Infineon}},
	month = feb,
	year = {2024},
}

@article{Lindholm2008,
	title = {{NVIDIA} {Tesla}: {A} {Unified} {Graphics} and {Computing} {Architecture}},
	volume = {28},
	doi = {10.1109/MM.2008.31},
	number = {2},
	journal = {IEEE Micro},
	author = {Lindholm, Erik and Nickolls, John and Oberman, Stuart and Montrym, John},
	year = {2008},
	keywords = {IEEE\_Xplore},
	pages = {39--55},
}

@article{Zhu2021,
	title = {Requirements-{Driven} {Automotive} {Electrical}/{Electronic} {Architecture}: {A} {Survey} and {Prospective} {Trends}},
	volume = {9},
	doi = {10.1109/ACCESS.2021.3093077},
	journal = {IEEE Access},
	author = {Zhu, Hailong and Zhou, Wei and Li, Zhiheng and Li, Li and Huang, Tao},
	year = {2021},
	keywords = {Computer architecture;Automotive engineering;Automobiles;Market research;Industries;Protocols;Ethernet;Automotive E/E architecture;automated driving;TSN;AUTOSAR;future trends, IEEE\_Xplore},
	pages = {100096--100112},
}

@incollection{Breebaart2004,
	address = {Dordrecht},
	title = {Features for {Audio} {Classification}},
	isbn = {978-94-017-0703-9},
	booktitle = {Algorithms in {Ambient} {Intelligence}},
	publisher = {Springer Netherlands},
	author = {Breebaart, Jeroen and McKinney, Martin F.},
	editor = {Verhaegh, W. F. J. and Aarts, E. and Korst, Jan},
	year = {2004},
	doi = {10.1007/978-94-017-0703-9_6},
	pages = {113--129},
}

@misc{GoogleTPU2024,
	title = {Introduction to {Cloud} {TPU}},
	url = {https://cloud.google.com/tpu/docs/intro-to-tpu},
	author = {{Google Cloud}},
	month = feb,
	year = {2024},
}

@misc{IntelFPGA2024,
	title = {Intel® {FPGAs} and {Programmable} {Devices}-{Intel}®  {FPGA}},
	url = {https://www.intel.com/content/www/us/en/products/programmable.html},
	author = {{Intel}},
	month = feb,
	year = {2024},
}

@inproceedings{Muntean2023,
	address = {Singapore},
	title = {Metrics for {Evaluating} {Classification} {Algorithms}},
	isbn = {978-981-19675-5-9},
	booktitle = {Education, {Research} and {Business} {Technologies}},
	publisher = {Springer Nature Singapore},
	author = {Muntean, Mihaelaand and Militaru, Florin-Daniel},
	editor = {Ciurea, Cristian and Pocatilu, Pauland and Filip, Florian Gheorghe},
	year = {2023},
	pages = {307--317},
}

@article{Ho2020,
	title = {Avoid {Oversimplifications} in {Machine} {Learning}: {Going} beyond the {Class}-{Prediction} {Accuracy}},
	volume = {1},
	issn = {26663899},
	doi = {10.1016/j.patter.2020.100025},
	number = {2},
	journal = {Patterns},
	author = {Ho, Sung Yang and Wong, Limsoon and Goh, Wilson Wen Bin},
	month = may,
	year = {2020},
	note = {Publisher: Cell Press},
	keywords = {DSML 5: Mainstream: Data science output is well understood and (nearly) universally adopted, PEL307, artificial intelligence, data science, machine learning, validation},
	pages = {100025},
}

@article{Hossin2015,
	title = {A {Review} {On} {Evaluation} {Metrics} {For} {Data} {Classification} {Evaluations}},
	volume = {5},
	doi = {10.5121/ijdkp.2015.5201},
	journal = {International Journal of Data Mining \& Knowledge Management Process},
	author = {Hossin, M and Sulaiman, M. N.},
	year = {2015},
	pages = {1--11},
}

@article{Rosenblatt1958,
	title = {The {Perceptron}: {A} {Probabilistic} {Model} for {Information} {Storage} and {Organization} in the {Brain}},
	volume = {65},
	doi = {10.1037/h0042519},
	number = {6},
	journal = {Psychological Review},
	author = {Rosenblatt, F},
	year = {1958},
	pages = {386--408},
}

@book{Sarkar2019,
	address = {Birmingham, England},
	edition = {1},
	title = {Ensemble {Machine} {Learning} {Cookbook}},
	isbn = {978-1-78913-660-9},
	publisher = {Packt Publishing},
	author = {Sarkar, Dipayan and Natarajan, Vijayalakshmi},
	month = jan,
	year = {2019},
}

@book{Hartshorn2016,
	title = {Machine {Learning} {With} {Random} {Forests} {And} {Decision} {Trees}: {A} {Visual} {Guide} {For} {Beginners}},
	publisher = {Kindle Direct Publishing},
	author = {Hartshorn, Scott},
	year = {2016},
}

@incollection{Genuer2020,
	address = {Cham},
	title = {Random {Forests}},
	isbn = {978-3-030-56484-1},
	booktitle = {Use {R}!},
	publisher = {Springer International Publishing},
	author = {Genuer, Robin and Poggi, Jean-Michel},
	year = {2020},
	pages = {33--55},
}

@article{Breiman2001,
	title = {Random {Forests}},
	volume = {45},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	number = {1},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	year = {2001},
	pages = {5--32},
}

@article{Hinton2006,
	title = {A {Fast} {Learning} {Algorithm} for {Deep} {Belief} {Nets}},
	volume = {18},
	issn = {0899-7667},
	doi = {10.1162/neco.2006.18.7.1527},
	number = {7},
	journal = {Neural Computation},
	author = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
	month = feb,
	year = {2006},
	pages = {1527--1554},
}

@book{Goodfellow2016,
	address = {London, England},
	edition = {1},
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	month = nov,
	year = {2016},
	note = {Series Title: Adaptive Computation and Machine Learning series},
}

@book{Bouguila2020,
	address = {Cham, Switzerland},
	edition = {1},
	title = {Mixture models and applications},
	isbn = {978-3-030-23878-0},
	language = {en},
	publisher = {Springer Nature},
	author = {Bouguila, Nizar and Fan, Wentao},
	editor = {Bouguila, Nizar and Fan, Wentao},
	month = aug,
	year = {2020},
	note = {Series Title: Unsupervised and Semi-Supervised Learning},
}

@article{Friedman1997,
	title = {Bayesian {Network} {Classifiers}},
	volume = {29},
	doi = {10.1023/A:1007465528199},
	number = {2},
	journal = {Machine Learning},
	author = {Friedman, Nir and Geiger, Dan and Goldszmidt, Moises},
	month = nov,
	year = {1997},
	pages = {131--163},
}

@article{Wickramasinghe2021,
	title = {Naive {Bayes}: applications, variations and vulnerabilities: a review of literature with code snippets for implementation},
	volume = {25},
	doi = {10.1007/s00500-020-05297-6},
	number = {3},
	journal = {Soft Computing},
	author = {Wickramasinghe, Indika and Kalutarage, Harsha},
	month = feb,
	year = {2021},
	pages = {2277--2293},
}

@article{Oppenheim2004,
	title = {From frequency to quefrency: a history of the cepstrum},
	volume = {21},
	doi = {10.1109/MSP.2004.1328092},
	number = {5},
	journal = {IEEE Signal Processing Magazine},
	author = {Oppenheim, A V and Schafer, R W},
	year = {2004},
	keywords = {Cepstrum;Frequency;History;Cepstral analysis;Convolution;Speech processing;Speech recognition;Nonlinear distortion;Signal processing;Deconvolution, IEEE\_Xplore},
	pages = {95--106},
}

@article{Bartsch2005,
	title = {Audio thumbnailing of popular music using chroma-based representations},
	volume = {7},
	doi = {10.1109/TMM.2004.840597},
	number = {1},
	journal = {IEEE Transactions on Multimedia},
	author = {Bartsch, M A and Wakefield, G H},
	year = {2005},
	keywords = {Multiple signal classification;Multimedia databases;Spatial databases;Pattern recognition;Costs;Speech;Redundancy;Time frequency analysis;Feature extraction;Multimedia systems;Audio summarization;chroma;feature extraction;musical structure;popular music},
	pages = {96--104},
}

@inproceedings{Harte2006,
	address = {New York, NY, USA},
	title = {Detecting harmonic change in musical audio},
	isbn = {1-59593-501-0},
	doi = {10.1145/1178723.1178727},
	booktitle = {Proceedings of the 1st {ACM} {Workshop} on {Audio} and {Music} {Computing} {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Harte, Christopher and Sandler, Mark and Gasser, Martin},
	year = {2006},
	note = {Series Title: AMCMM '06},
	keywords = {audio, harmonic, music, pitch space, segmentation},
	pages = {21--26},
}

@inproceedings{Jiang2002,
	title = {Music type classification by spectral contrast feature},
	volume = {1},
	doi = {10.1109/ICME.2002.1035731},
	booktitle = {{IEEE} {International} {Conference} on {Multimedia} and {Expo}},
	author = {Jiang, Dan-Ning and Lu, Lie and Zhang, Hong-Jiang and Tao, Jian-Hua and Cai, Lian-Hong},
	year = {2002},
	keywords = {IEEE\_Xplore, Multiple signal classification;Mel frequency cepstral coefficient;Cepstral analysis;Asia;Spatial databases;Hidden Markov models;Computer science;Technology management;Modems;History},
	pages = {113--116},
}

@book{Klapuri2006,
	address = {New York, NY},
	edition = {1},
	title = {Signal processing methods for music transcription},
	isbn = {978-0-387-30667-4},
	language = {en},
	publisher = {Springer},
	author = {Klapuri, Anssi and Davy, Manuel},
	editor = {Klapuri, Anssi and Davy, Manuel},
	month = may,
	year = {2006},
}

@article{Peeters2004,
	title = {A large set of audio features for sound description (similarity and classification) in the {CUIDADO} project},
	volume = {54},
	url = {http://recherche.ircam.fr/anasyn/peeters/ARTICLES/Peeters_2003_cuidadoaudiofeatures.pdf},
	number = {0},
	urldate = {2024-01-28},
	journal = {CUIDADO Ist Project Report},
	author = {Peeters, Geoffroy},
	year = {2004},
	pages = {1--25},
}

@book{Giannakopoulos2014,
	address = {Oxford, UK},
	edition = {1},
	title = {Introduction to audio analysis},
	isbn = {978-0-08-099388-1},
	publisher = {Academic Press},
	author = {Giannakopoulos, Theodoros and Pikrakis, Aggelos},
	month = apr,
	year = {2014},
}

@book{Park2008,
	address = {Singapore, Singapore},
	title = {Introduction to digital signal processing: {Computer} musically speaking},
	isbn = {978-981-279-027-9},
	publisher = {World Scientific Publishing},
	author = {Park, Tae Hong},
	month = may,
	year = {2008},
}

@book{Gold2011,
	address = {Hoboken, NJ},
	edition = {2},
	title = {Speech and audio signal processing},
	isbn = {978-0-470-19536-9},
	language = {en},
	publisher = {Wiley-Blackwell},
	author = {Gold, Ben and Morgan, Nelson and Ellis, Dan},
	month = aug,
	year = {2011},
}

@book{Zhang2010,
	address = {New York, NY},
	edition = {1},
	title = {Content-based audio classification and retrieval for audiovisual data parsing},
	isbn = {978-1-4419-4878-6},
	language = {en},
	publisher = {Springer},
	author = {Zhang, Tong and C. Jay Kuo, C},
	month = dec,
	year = {2010},
	note = {Series Title: The Springer International Series in Engineering and Computer Science},
}

@article{Preece2009,
	title = {Activity {Identification} {Using} {Body}-{Mounted} {Sensors} — {A} {Review} of {Classification} {Techniques}},
	volume = {30},
	doi = {10.1088/0967-3334/30/4/R01},
	journal = {Physiological measurement},
	author = {Preece, Stephen and Goulermas, John and Kenney, Laurence and Howard, Deandra and Meijer, Kenneth and Crompton, Robin},
	month = jan,
	year = {2009},
	pages = {R1--33},
}

@book{Smith2013,
	title = {Digital signal processing: {A} practical guide for engineers and scientists},
	isbn = {978-0-08-047732-9},
	language = {en},
	publisher = {Elsevier},
	author = {Smith, Steven},
	month = oct,
	year = {2013},
}

@book{Zoelzer2008,
	address = {Chichester, UK},
	title = {Digital {Audio} {Signal} {Processing}},
	isbn = {978-0-470-99785-7},
	abstract = {A fully updated second edition of the excellent Digital Audio},
	publisher = {Wiley},
	author = {Zoelzer, Udo},
	month = jun,
	year = {2008},
}

@book{Debnath2014,
	address = {Secaucus, NJ},
	edition = {2},
	title = {Wavelet transforms and their applications},
	isbn = {978-0-8176-8417-4},
	language = {en},
	publisher = {Birkhauser Boston},
	author = {Debnath, Lokenath and Shah, Firdous Ahmad},
	month = nov,
	year = {2014},
}

@book{Pelgrom2018,
	address = {Cham, Switzerland},
	edition = {3},
	title = {Analog-to-{Digital} {Conversion}},
	isbn = {978-3-319-44970-8},
	language = {en},
	publisher = {Springer International Publishing},
	author = {Pelgrom, Marcel},
	month = jun,
	year = {2018},
}

@misc{WAYMO2023,
	title = {The {Waymo} {Driver}'s {Rapid} {Learning} {Curve}},
	url = {https://waymo.com/blog/2023/08/the-waymo-drivers-rapid-learning-curve/},
	urldate = {2024-01-21},
	author = {{WAYMO}},
	month = jan,
	year = {2023},
	keywords = {Auto, Autonomous vehicle, Microphone},
}

@patent{Ferguson2014,
	address = {USA},
	title = {Controlling autonomous vehicle using audio data},
	nationality = {USA},
	assignee = {Google Inc., Mountain View, CA (US)},
	number = {US8676427B1},
	author = {Ferguson, David I. and Zhu, Jiajun},
	month = mar,
	year = {2014},
	note = {Publisher: United States Patent},
	pages = {1--12},
}

@article{Tang2018,
	title = {Improved {Convolutional} {Neural} {Networks} for {Acoustic} {Event} {Classification}},
	volume = {78},
	doi = {10.1007/s11042-018-6991-4},
	journal = {Multimedia Tools and Applications},
	author = {Tang, Guichen and Liang, Ruiyu and Xie, Yue and Bao, Yongqiang and Wang, Shijia},
	year = {2018},
	keywords = {CNN, Delta, ESR, Semantic\_Scholar},
	pages = {15801 -- 15816},
}

@article{Li2018,
	title = {An ensemble stacked convolutional neural network model for environmental event sound recognition},
	volume = {8},
	doi = {10.3390/app8071152},
	number = {7},
	journal = {Applied Sciences (Switzerland)},
	author = {Li, Shaobo and Yao, Yong and Hu, Jie and Liu, Guokai and Yao, Xuemei and Hu, Jianjun},
	year = {2018},
	keywords = {CNN, ESR, SCOPUS},
}

@article{Su2019,
	title = {Environment sound classification using a two-stream {CNN} based on decision-level fusion},
	volume = {19},
	doi = {10.3390/s19071733},
	abstract = {With the popularity of using deep learning-based models in various categorization problems and their proven robustness compared to conventional methods, a growing number of researchers have exploited such methods in environment sound classification tasks in recent years. However, the performances of existing models use auditory features like log-mel spectrogram (LM) and mel frequency cepstral coefficient (MFCC), or raw waveform to train deep neural networks for environment sound classification (ESC) are unsatisfactory. In this paper, we first propose two combined features to give a more comprehensive representation of environment sounds Then, a fourfour-layer convolutional neural network (CNN) is presented to improve the performance of ESC with the proposed aggregated features. Finally, the CNN trained with different features are fused using the Dempster–Shafer evidence theory to compose TSCNN-DS model. The experiment results indicate that our combined features with the four-layer CNN are appropriate for environment sound taxonomic problems and dramatically outperform other conventional methods. The proposed TSCNN-DS model achieves a classification accuracy of 97.2\%, which is the highest taxonomic accuracy on UrbanSound8K datasets compared to existing models.},
	number = {7},
	journal = {Sensors (Switzerland)},
	author = {Su, Y. and Zhang, K. and Wang, J. and Madani, K.},
	year = {2019},
	keywords = {CNN, ESR, Feature aggregation, SCOPUS},
}

@article{Inik2023,
	title = {{CNN} hyper-parameter optimization for environmental sound classification},
	volume = {202},
	issn = {0003-682X},
	url = {https://www.sciencedirect.com/science/article/pii/S0003682X22005424},
	doi = {https://doi.org/10.1016/j.apacoust.2022.109168},
	abstract = {Environmental sounds are being used widely in our lives. It is especially used in tasks such as managing smart cities, location determination, surveillance systems, machine hearing, and environmental monitoring. The main method for this, environmental sound classification (ESC), has been increasingly studied in recent years. However, the classification of these sounds is more difficult than other sounds because there are too many parameters that generate noise. The study tried to find the convolutional neural network (CNN) model that gave the highest accuracy for ESC tasks with the optimization of hyper-parameters. For this purpose, the Particle Swarm Optimization (PSO) algorithm was rearranged to represent the CNN architecture. Thus, the hyper-parameters in CNN are represented exactly without any transformation during optimization. Studies were carried out on the ESC-10, ESC-50, and Urbansound8k data sets, which are state-of-art for ESC tasks. Some data augmentation techniques have been used for data sets in the training of CNN models. The CNN models, which were obtained with PSO, achieved success rates of 98.64 \% for ESC-10, 93.71 \% for ESC-50, and 98.45 \% for Urbansound8k, respectively. These results are the best accuracy values obtained with the pure CNN model when compared with previous studies. As a result, it has been made possible to automatically design CNN models for the classification of urban sounds, giving high classification accuracy. Thus, researchers who do not know much about CNN design can use this method in their desired datasets without the need for expert knowledge.},
	journal = {Applied Acoustics},
	author = {İnik, Özkan},
	year = {2023},
	keywords = {CNN, ESR, Not\_in\_related\_work, Science\_Direct},
	pages = {109168},
}

@misc{BBC2023,
	title = {Complete {Sound} {Effects} {Library}},
	url = {https://sound-effects.bbcrewind.co.uk/},
	urldate = {2023-12-09},
	author = {{BBC}},
	year = {2023},
	keywords = {Database, Not\_in\_related\_work},
}

@inproceedings{Font_freesound2013,
	address = {New York, NY, USA},
	title = {Freesound {Technical} {Demo}},
	isbn = {978-1-4503-2404-5},
	doi = {10.1145/2502081.2502245},
	abstract = {Freesound is an online collaborative sound database where people with diverse interests share recorded sound samples under Creative Commons licenses. It was started in 2005 and it is being maintained to support diverse research projects and as a service to the overall research and artistic community. In this demo we want to introduce Freesound to the multimedia community and show its potential as a research resource. We begin by describing some general aspects of Freesound, its architecture and functionalities, and then explain potential usages that this framework has for research applications.},
	booktitle = {Proceedings of the 21st {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Font, Frederic and Roma, Gerard and Serra, Xavier},
	year = {2013},
	note = {Series Title: MM '13},
	keywords = {ACM\_digital\_library, Database, Not\_in\_related\_work},
	pages = {411--412},
}

@inproceedings{Chaturvedi2023,
	title = {Classification of {Sound} using {Convolutional} {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/document/10072823},
	doi = {10.1109/IC3I56241.2022.10072823},
	urldate = {2023-10-08},
	booktitle = {2022 5th {International} {Conference} on {Contemporary} {Computing} and {Informatics} ({IC3I})},
	author = {Chaturvedi, Abhay and Yadav, Suman Avdhesh and Salman, Hayder M and Goyal, Himanshu Rai and Gebregziabher, Haftom and Rao, A Kakoli},
	year = {2022},
	keywords = {CNN, ESR, IEEE\_Xplore, Not\_in\_related\_work},
	pages = {1015--1019},
}

@incollection{Veeraraghavan2020,
	address = {Cham},
	title = {{SoC} {Based} {Acoustic} {Controlled} {Semi} {Autonomous} {Driving} {System}},
	isbn = {978-3-030-24642-6},
	url = {http://link.springer.com/10.1007/978-3-030-24643-3_71},
	abstract = {Autonomous driving car provide various features such as Adaptive Cruise Control, Autonomous Emergency Breaking, Lane Detection etc. which all environment mapping using image processing, Lidar, radar, GPS and computer vision. Features like image processing require lot of data processing power and hence can be expensive. In country like where people can afford expensive autonomous car with level 4 or 5 autonomous driving level, and with traffic being highly unreliable, a different system is required which is suitable for Indian road. In this paper, we proposed that is a level 3 autonomous driving vehicle and operates based on the processing of the acoustic, sound and noises from the surrounding. The audio processing is done using Xilinx Zynq FPGA SoC and ICS43432 microphone as hardware, running on convolutional neural network and Fuzzy control. The system on chip (SoC) is the future of embedded system design. An FPGA based SoC platform offers many advantages, including high integration and easy field upgrades of entire systems.},
	urldate = {2023-03-23},
	booktitle = {Proceeding of the {International} {Conference} on {Computer} {Networks}, {Big} {Data} and {IoT} ({ICCBI} - 2018)},
	publisher = {Springer International Publishing},
	author = {Veeraraghavan, A. K. and Ranga Charan, S.},
	editor = {Pandian, A. Pasumpon and Senjyu, Tomonobu and Islam, Syed Mohammed Shamsul and Wang, Haoxiang},
	year = {2020},
	doi = {10.1007/978-3-030-24643-3_71},
	keywords = {Autonomous vehicle, ESR, Embedded system, Microcontroller, SCOPUS},
	pages = {591--599},
}

@misc{ISPOT2020,
	title = {I-{SPOT}},
	url = {https://i-spot-project.eu/},
	urldate = {2024-01-12},
	author = {{I-SPOT}},
	month = jan,
	year = {2020},
	keywords = {Autonomous vehicle, ESR, Embedded system},
}

@article{Lamrini2023,
	title = {Evaluating the {Performance} of {Pre}-{Trained} {Convolutional} {Neural} {Network} for {Audio} {Classification} on {Embedded} {Systems} for {Anomaly} {Detection} in {Smart} {Cities}},
	volume = {23},
	doi = {10.3390/s23136227},
	number = {13},
	journal = {Sensors},
	author = {Lamrini, M and Chkouri, M Y and Touhafi, A},
	year = {2023},
	keywords = {Anomaly detection, CNN, ESR, Embedded system, SCOPUS},
}

@article{Su2020,
	title = {Performance analysis of multiple aggregated acoustic features for environment sound classification},
	volume = {158},
	doi = {10.1016/j.apacoust.2019.107050},
	journal = {Applied Acoustics},
	author = {Su, Yu and Zhang, Ke and Wang, Jingyu and Zhou, Daming and Madani, Kurosh},
	year = {2020},
	keywords = {CNN, ESR, Feature aggregation, SCOPUS},
}

@article{Chu2023,
	title = {A {CNN} {Sound} {Classification} {Mechanism} {Using} {Data} {Augmentation}},
	volume = {23},
	doi = {10.3390/s23156972},
	number = {15},
	journal = {Sensors},
	author = {Chu, H.-C. and Zhang, Y.-L. and Chiang, H.-C.},
	year = {2023},
	keywords = {CNN, Data augmentation, ESR, SCOPUS},
}

@inproceedings{Park2019,
	title = {Specaugment: {A} simple data augmentation method for automatic speech recognition},
	volume = {2019-September},
	doi = {10.21437/Interspeech.2019-2680},
	booktitle = {Proceedings of the {Annual} {Conference} of the {International} {Speech} {Communication} {Association}, {INTERSPEECH}},
	author = {Park, Daniel S and Chan, William and Zhang, Yu and Chiu, Chung-Cheng and Zoph, Barret and Cubuk, Ekin D and Le, Quoc V},
	year = {2019},
	keywords = {Data augmentation, Speech recognition},
	pages = {2613--2617},
}

@inproceedings{Bansal2022,
	title = {Environmental {Sound} {Classification} using {Hybrid} {Ensemble} {Model}},
	volume = {218},
	doi = {10.1016/j.procs.2023.01.024},
	booktitle = {Procedia {Computer} {Science}},
	author = {Bansal, A and Garg, N K},
	year = {2022},
	keywords = {ESR, Ensemble learning, Feature extraction, SCOPUS},
	pages = {418--428},
}

@article{Alias2016,
	title = {A review of physical and perceptual feature extraction techniques for speech, music and environmental sounds},
	volume = {6},
	doi = {10.3390/app6050143},
	number = {5},
	journal = {Applied Sciences},
	author = {Alías, Francesc and Socoró, Joan Claudi and Sevillano, Xavier},
	year = {2016},
	keywords = {Feature extraction, Review, SCOPUS},
}

@inproceedings{Yin2023,
	title = {Real-{Time} {Acoustic} {Perception} for {Automotive} {Applications}},
	doi = {10.23919/DATE56975.2023.10137209},
	booktitle = {2023 {Design}, {Automation} \& {Test} in {Europe} {Conference} \& {Exhibition} ({DATE})},
	author = {Yin, J and Damiano, S and Verhelst, M and Waterschoot, T van and Guntoro, A},
	year = {2023},
	keywords = {Autonomous vehicle, ESR, IEEE\_Xplore, Regular passenger car},
	pages = {1--6},
}

@article{Pandya2021,
	title = {Ambient acoustic event assistive framework for identification, detection, and recognition of unknown acoustic events of a residence},
	volume = {47},
	issn = {1474-0346},
	url = {https://www.sciencedirect.com/science/article/pii/S147403462030207X},
	doi = {https://doi.org/10.1016/j.aei.2020.101238},
	abstract = {In recent times, Ambient Assisted Living has emerged as Smart Living. Smart living is a subset of ambient intelligence, which uses the latest technologies, intellectual processes, and ambient intelligent methodologies to enable house residents to live independently with a virtual companion 24 × 7. Typically, these residents are highly engrossed in the daily routine activities that they tend to ignore certain acoustic events attributing them to the white noise caused due to tap water leakage, flush water leakage, the acoustics of door opening/closing, cupboard opening/closing, curtain opening/closing, television, shower, radio, chair and many more. These unattended events lead to a waste of critical energy resources such as electricity, water, and gas and may cause accidents in some cases. For the conducted experiments, a customized dataset termed as “unknown-2000” and ESC-50 has been used, which has more than 2000 audio sound classification samples. The customized dataset is used for the conducted experiments, consisting of various length acoustic events ranging from 2 s to 10 s. In the proposed review, we have identified, analyzed, and evaluated resident acoustic events using Librosa machine learning libraries, texture analysis using LBP methodology, LSTM-CNN, SVM, KNN, LSTM, Bi-LSTM, and Decision Tree-based classification approaches. Furthermore, in the proposed approach, based on the conducted rigorous and detailed analysis, we are also envisioning the prospective ways to enhance smart living concepts by proposing a novel Acoustic Event Detection and Classification System. The investigation results validate the success of the proposed approach. The obtained results indicate that the customized version of the LSTM-CNN based classification approach used in the conducted experiment has outperformed all the other customized classification approaches, such as SVM, KNN-based classification, C4.5 decision tree-based classification, LSTM, and Bi-LSTM based classification. The LSTM-CNN based classification model has achieved an average value of approximately 0.77 and a standard deviation of 0.2295. Furthermore, the obtained experiential results show that the proposed approach has produced a good performance in various noisy conditions such as SNR0, SNR3, SNR6, SNR9, SNR12, and SNR15. The system classification accuracy has been enhanced to 77\% for various acoustic events of a residence. In the end, a detailed comparison of LBP and without LBP approaches has been carried out, which proves that the combination of LBP and LSTM-CNN classification approach provides better results than without the LBP classification approach. The proposed Ambient Acoustic Event Assistive Framework is a cost-effective alternative due to the use of low-cost microphone sensors in the conducted experiments.},
	journal = {Advanced Engineering Informatics},
	author = {Pandya, Sharnil and Ghayvat, Hemant},
	year = {2021},
	keywords = {CNN, ESR, Embedded system, Home monitoring residential assistance, LSTM, Microcontroller, Science\_Direct},
	pages = {101238},
}

@inproceedings{Saraubon2018,
	title = {A {Smart} {System} for {Elderly} {Care} using {IoT} and {Mobile} {Technologies}},
	doi = {10.1145/3301761.3301769},
	booktitle = {{ICSEB} 18},
	author = {Saraubon, Kobkiat and Anurugsa, Keattisuk and Kongsakpaibul, Adichart},
	year = {2018},
	keywords = {Semantic\_Scholar},
}

@article{Fukuyama2022,
	title = {Identification of {Respiratory} {Sounds} {Collected} from {Microphones} {Embedded} in {Mobile} {Phones}},
	volume = {11},
	doi = {10.14326/abe.11.58},
	journal = {Advanced Biomedical Engineering},
	author = {Fukuyama, Keita and Sugiyama, Osamu and Chin, Kazuo and Satou, Susumu and Matsumoto, Shigemi and Muto, Manabu},
	year = {2022},
	keywords = {SCOPUS},
	pages = {58 -- 67},
}

@inproceedings{Jeong2022,
	title = {Constructing an {Audio} {Dataset} of {Construction} {Equipment} from {Online} {Sources} for {Audio}-{Based} {Recognition}},
	volume = {2022-December},
	doi = {10.1109/WSC57314.2022.10015388},
	booktitle = {Proceedings - {Winter} {Simulation} {Conference}},
	author = {Jeong, Gilsu and Ahn, Changbum R and Park, Moonseo},
	year = {2022},
	keywords = {SCOPUS},
	pages = {2354 -- 2364},
}

@article{Branding2023,
	title = {Towards noise robust acoustic insect detection: from the lab to the greenhouse},
	doi = {10.1007/s13218-023-00812-x},
	journal = {KI - Kunstliche Intelligenz},
	author = {Branding, Jelto and von Hörsten, Dieter and Wegener, Jens Karl and Böckmann, Elias and Hartung, Eberhard},
	year = {2023},
	keywords = {SCOPUS},
}

@article{Jeantet2023,
	title = {Improving deep learning acoustic classifiers with contextual information for wildlife monitoring},
	volume = {77},
	doi = {10.1016/j.ecoinf.2023.102256},
	journal = {Ecological Informatics},
	author = {Jeantet, Lorène and Dufourq, Emmanuel},
	year = {2023},
	keywords = {SCOPUS},
}

@inproceedings{Huang2023,
	title = {"{Not} {There} {Yet}": {Feasibility} and {Challenges} of {Mobile} {Sound} {Recognition} to {Support} {Deaf} and {Hard}-of-{Hearing} {People}},
	doi = {10.1145/3597638.3608431},
	booktitle = {{ASSETS} 2023 - {Proceedings} of the 25th {International} {ACM} {SIGACCESS} {Conference} on {Computers} and {Accessibility}},
	author = {Huang, Jeremy Zhengqi and Chhabria, Hriday and Jain, Dhruv},
	year = {2023},
	keywords = {SCOPUS},
}

@article{VidaaVila2020,
	title = {Low-{Cost} {Distributed} {Acoustic} {Sensor} {Network} for {Real}-{Time} {Urban} {Sound} {Monitoring}},
	volume = {9},
	issn = {2079-9292},
	doi = {10.3390/electronics9122119},
	number = {12},
	urldate = {2024-01-06},
	journal = {Electronics},
	author = {Vidaña-Vila, Ester and Navarro, Joan and Borda-Fortuny, Cristina and Stowell, Dan and Alsina-Pagès, Rosa Ma},
	year = {2020},
	keywords = {ESR, Embedded system, Microcontroller, Semantic\_Scholar, Urban environment},
}

@inproceedings{Sharma2021,
	address = {Singapore},
	title = {Emergency detection with environment sound using deep convolutional neural networks},
	isbn = {978-981-15-5859-7},
	doi = {DOI: 10.1007/978-981-15-5859-7_14},
	booktitle = {Proceedings of {Fifth} {International} {Congress} on {Information} and {Communication} {Technology}},
	publisher = {Springer Singapore},
	author = {Sharma, Jivitesh and Granmo, Ole-Christoffer and Goodwin, Morten},
	year = {2021},
	keywords = {CNN, Deep learning, ESR, Emergency detection, Semantic\_Scholar},
	pages = {144--154},
}

@article{Tripathi2021,
	title = {Self-supervised learning for {Environmental} {Sound} {Classification}},
	volume = {182},
	issn = {0003-682X},
	doi = {https://doi.org/10.1016/j.apacoust.2021.108183},
	journal = {Applied Acoustics},
	author = {Tripathi, Achyut Mani and Mishra, Aakansha},
	year = {2021},
	keywords = {ESR, SCOPUS, Self supervised learning, Transfer learning},
	pages = {108183},
}

@article{Tran2020,
	title = {Acoustic-{Based} {Emergency} {Vehicle} {Detection} {Using} {Convolutional} {Neural} {Networks}},
	volume = {8},
	doi = {10.1109/ACCESS.2020.2988986},
	journal = {IEEE Access},
	author = {Tran, Van-Thuan and Tsai, Wei-Ho},
	year = {2020},
	keywords = {CNN, Deep learning, IEEE\_Xplore, Siren detection},
	pages = {75702--75713},
}

@article{Marchegiani2022,
	title = {Listening for {Sirens}: {Locating} and {Classifying} {Acoustic} {Alarms} in {City} {Scenes}},
	volume = {23},
	doi = {10.1109/TITS.2022.3158076},
	number = {10},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Marchegiani, Letizia and Newman, Paul},
	year = {2022},
	keywords = {IEEE\_Xplore, Regular passenger car, Siren detection},
	pages = {17087--17096},
}

@article{Sun2021,
	title = {Emergency {Vehicles} {Audio} {Detection} and {Localization} in {Autonomous} {Driving}},
	volume = {abs/2109.14797},
	journal = {ArXiv},
	author = {Sun, Hongyi and Liu, Xinyi and Xu, Kecheng and Miao, Jinghao and Luo, Qi},
	year = {2021},
	note = {arXiv: 238226804},
	keywords = {Autonomous vehicle, Siren detection},
}

@inproceedings{Shabtai2019,
	address = {Paris, France},
	title = {Detecting the direction of emergency vehicle sirens with microphones},
	doi = {10.25836/sasp.2019.22},
	booktitle = {{EAA} {Spatial} {Audio} {Signal} {Processing} {Symposium}},
	author = {Shabtai, Noam R and Tzirkel, Eli},
	month = sep,
	year = {2019},
	keywords = {Autonomous vehicle, Siren detection},
	pages = {137--142},
}

@misc{BOSCH2024,
	title = {Embedded siren detection {\textbar} {Bosch} {Global}},
	url = {https://www.bosch.com/stories/embedded-siren-detection/},
	author = {{BOSCH}},
	month = jan,
	year = {2024},
	keywords = {Sensor, Siren detection},
}

@phdthesis{Nordby2019,
	address = {Oslo, Norway},
	title = {Environmental sound classification on microcontrollers using {Convolutional} {Neural} {Networks}},
	url = {http://hdl.handle.net/11250/2611624},
	urldate = {2023-10-07},
	school = {Norwegian University of Life Sciences, Ås},
	author = {Nordby, Jon Opedal},
	year = {2019},
	keywords = {CNN, ESR, Microcontroller, Semantic\_Scholar},
}

@phdthesis{Abreha2014,
	title = {An environmental audio-based context recognition system using smartphones},
	url = {http://essay.utwente.nl/66444/},
	author = {Abreha, Gebremedhin Teklemariam},
	month = aug,
	year = {2014},
	keywords = {ESR, Embedded system, Microcontroller},
}

@inproceedings{Bountourakis2015,
	address = {New York, NY, USA},
	title = {Machine {Learning} {Algorithms} for {Environmental} {Sound} {Recognition}: {Towards} {Soundscape} {Semantics}},
	isbn = {978-1-4503-3896-7},
	doi = {10.1145/2814895.2814905},
	abstract = {This paper investigates methods aiming at the automatic recognition and classification of discrete environmental sounds, for the purpose of subsequently applying these methods to the recognition of soundscapes. Research in audio recognition has traditionally focused on the domains of speech and music. Comparatively little research has been done towards recognizing non-speech environmental sounds. For this reason, in this paper, we apply existing techniques that have been proved efficient in the other two domains. These techniques are comprehensively compared to determine the most appropriate one for addressing the problem of environmental sound recognition.},
	booktitle = {Proceedings of the {Audio} {Mostly} 2015 on {Interaction} {With} {Sound}},
	publisher = {Association for Computing Machinery},
	author = {Bountourakis, Vasileios and Vrysis, Lazaros and Papanikolaou, George},
	year = {2015},
	note = {Series Title: AM '15},
	keywords = {ESR, Feature extraction, Machine learning, Semantic\_Scholar},
}

@article{Luz2021,
	title = {Ensemble of handcrafted and deep features for urban sound classification},
	volume = {175},
	doi = {10.1016/j.apacoust.2020.107819},
	abstract = {The urban sound classification has a strong relation with feature extraction. In this paper, we present a compact and effective representation capable of characterizing different urban sounds based on deep and handcrafted features combination. To this end, we propose a small parameter space CNN model to extract deep features that are combined with handcrafted features extracted from audio signals. Then, we apply a feature selection step to reduce feature dimensionality and to investigate handcrafted features that enrich deep features to better discriminate between urban sounds. The feature selection experiment results indicate that associating perceptual, static, and physical features with deep features improves the classification performance and allows a dimension reduction up to 62.32\% for the combined descriptors. The proposed descriptors achieve a classification accuracy of 86.2\% for the ESC (urban noises) dataset and 96.16\% for the UrbanSound8K dataset, outperforming most of the state-of-the-art CNN models for urban sound classification.},
	journal = {Applied Acoustics},
	author = {Luz, J.S. and Oliveira, M.C. and Araújo, F.H.D. and Magalhães, D.M.V.},
	year = {2021},
	keywords = {CNN, ESR, Feature aggregation, Machine learning, SCOPUS},
}

@article{Mmushtaq2020,
	title = {Environmental sound classification using a regularized deep convolutional neural network with data augmentation},
	volume = {167},
	issn = {0003-682X},
	doi = {10.1016/j.apacoust.2020.107389},
	abstract = {The adoption of the environmental sound classification (ESC) tasks increases very rapidly over recent years due to its broad range of applications in our daily routine life. ESC is also known as Sound Event Recognition (SER) which involves the context of recognizing the audio stream, related to various environmental sounds. Some frequent and common aspects like non-uniform distance between acoustic source and microphone, the difference in the framework, presence of numerous sounds sources in audio recordings and overlapping various sound events make this ESC problem much complex and complicated. This study is to employ deep convolutional neural networks (DCNN) with regularization and data enhancement with basic audio features that have verified to be efficient on ESC tasks. In this study, the performance of DCNN with max-pooling (Model-1) and without max-pooling (Model-2) function are examined. Three audio attribute extraction techniques, Mel spectrogram (Mel), Mel Frequency Cepstral Coefficient (MFCC) and Log-Mel, are considered for the ESC-10, ESC-50, and Urban sound (US8K) datasets. Furthermore, to avoid the risk of overfitting due to limited numbers of data, this study also introduces offline data augmentation techniques to enhance the used datasets with a combination of L2 regularization. The performance evaluation illustrates that the best accuracy attained by the proposed DCNN without max-pooling function (Model-2) and using Log-Mel audio feature extraction on those augmented datasets. For ESC-10, ESC-50 and US8K, the highest achieved accuracies are 94.94\%, 89.28\%, and 95.37\% respectively. The experimental results show that the proposed approach can accomplish the best performance on environment sound classification problems.},
	journal = {Applied Acoustics},
	author = {Mushtaq, Zohaib and Su, Shun-Feng},
	year = {2020},
	keywords = {CNN, Data augmentation, ESR, Science\_Direct},
	pages = {107389},
}

@article{Vandendriessche2021,
	title = {Environmental {Sound} {Recognition} on {Embedded} {Systems}: {From} {FPGAs} to {TPUs}},
	volume = {10},
	doi = {10.3390/electronics},
	abstract = {Citation: Vandendriessche, J.; Wouters, N.; da Silva, B.; Lamrini, M.; Chkouri, M.Y.; Touhafi, A. Environmental Sound Recognition on Embedded Systems: From FPGAs to TPUs. Electronics 2021, 10, 2622. Abstract: In recent years, Environmental Sound Recognition (ESR) has become a relevant capability for urban monitoring applications. The techniques for automated sound recognition often rely on machine learning approaches, which have increased in complexity in order to achieve higher accuracy. Nonetheless, such machine learning techniques often have to be deployed on resource and power-constrained embedded devices, which has become a challenge with the adoption of deep learning approaches based on Convolutional Neural Networks (CNNs). Field-Programmable Gate Arrays (FPGAs) are power efficient and highly suitable for computationally intensive algorithms like CNNs. By fully exploiting their parallel nature, they have the potential to accelerate the inference time as compared to other embedded devices. Similarly, dedicated architectures to accelerate Artificial Intelligence (AI) such as Tensor Processing Units (TPUs) promise to deliver high accuracy while achieving high performance. In this work, we evaluate existing tool flows to deploy CNN models on FPGAs as well as on TPU platforms. We propose and adjust several CNN-based sound classifiers to be embedded on such hardware accelerators. The results demonstrate the maturity of the existing tools and how FPGAs can be exploited to outperform TPUs.},
	number = {21},
	journal = {Electronics (Switzerland)},
	author = {Vandendriessche, Jurgen and Wouters, Nick and Da Silva, Bruno and Lamrini, Mimoun and Chkouri, Mohamed Yassin and Touhafi, Abdellah},
	year = {2021},
	keywords = {CNN, ESR, Embedded system, Machine learning},
	pages = {1--31},
}

@article{Lhoest2021,
	title = {{MosAIc}: {A} classical machine learning multi-classifier based approach against deep learning classifiers for embedded sound classification},
	volume = {11},
	doi = {10.3390/app11188394},
	abstract = {Environmental Sound Recognition has become a relevant application for smart cities. Such an application, however, demands the use of trained machine learning classifiers in order to categorize a limited set of audio categories. Although classical machine learning solutions have been proposed in the past, most of the latest solutions that have been proposed toward automated and accurate sound classification are based on a deep learning approach. Deep learning models tend to be large, which can be problematic when considering that sound classifiers often have to be embedded in resource constrained devices. In this paper, a classical machine learning based classifier called MosAIc, and a lighter Convolutional Neural Network model for environmental sound recognition, are proposed to directly compete in terms of accuracy with the latest deep learning solutions. Both approaches are evaluated in an embedded system in order to identify the key parameters when placing such applications on constrained devices. The experimental results show that classical machine learning classifiers can be combined to achieve similar results to deep learning models, and even outperform them in accuracy. The cost, however, is a larger classification time.},
	number = {18},
	journal = {Applied Sciences (Switzerland)},
	author = {Lhoest, L. and Lamrini, M. and Vandendriessche, J. and Wouters, N. and da Silva, B. and Chkouri, M.Y. and Touhafi, A.},
	year = {2021},
	keywords = {ESR, Embedded system, Machine learning, SCOPUS},
}

@article{Silva2019,
	title = {Evaluation of classical {Machine} {Learning} techniques towards urban sound recognition on embedded systems},
	volume = {9},
	doi = {10.3390/app9183885},
	number = {18},
	urldate = {2022-10-17},
	journal = {Applied Sciences (Switzerland)},
	author = {Silva, Bruno da and Happi, Axel W. and Braeken, An and Touhafi, Abdellah},
	month = sep,
	year = {2019},
	keywords = {ESR, Embedded system, Machine learning},
	pages = {3885},
}

@incollection{Shreyas2020,
	title = {Chapter 7 - {Trends} of {Sound} {Event} {Recognition} in {Audio} {Surveillance}: {A} {Recent} {Review} and {Study}},
	isbn = {978-0-12-816385-6},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128163856000076},
	abstract = {Identification of environment sounds plays a key role in security and surveillance aspects. In this paper, we present a review and recent advances in a sound event recognition (SER) problem and discuss different deep learning approaches applied for SER. SER is the process of extracting the pertinent features from the given sound and classify it into various categories. This SER task has attracted a lot of attention in recent times such as smart homes, cough sound detection, etc. Apart from other surveys, this survey focuses on SER task and carried out an extensive study of few baseline methods on Environmental Sound Classification 50 data set, which consists of 50 sound classes. From this data set, we have extracted Mel-frequency cepstral coefficient features and fed as input to classifiers such as support vector machine, artificial neural network, and convolution neural network (CNN). Based on this study and experimentation, we explore various pros and cons over this SER task. Deep learning techniques are used to improve the recognition rate of SER task. It is evident from the results reported in the DCASE challenge 2016–2017 data set that CNNs are proven to be effective for SER.},
	booktitle = {The {Cognitive} {Approach} in {Cloud} {Computing} and {Internet} of {Things} {Technologies} for {Surveillance} {Tracking} {Systems}},
	publisher = {Academic Press},
	author = {Shreyas, N and Venkatraman, M and Malini, S and Chandrakala, S},
	editor = {Peter, Dinesh and Alavi, Amir H and Javadi, Bahman and Fernandes, Steven L},
	year = {2020},
	doi = {https://doi.org/10.1016/B978-0-12-816385-6.00007-6},
	note = {Series Title: Intelligent Data-Centric Systems},
	keywords = {Acoustic event recognition, CNN, Deep learning, ESR, Science\_Direct},
	pages = {95--106},
}

@article{Mesaros2019,
	title = {Sound {Event} {Detection} in the {DCASE} 2017 {Challenge}},
	volume = {27},
	issn = {2329-9290},
	url = {https://ieeexplore.ieee.org/document/8673582/},
	doi = {10.1109/TASLP.2019.2907016},
	number = {6},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Mesaros, Annamaria and Diment, Aleksandr and Elizalde, Benjamin and Heittola, Toni and Vincent, Emmanuel and Raj, Bhiksha and Virtanen, Tuomas},
	month = jun,
	year = {2019},
	keywords = {Acoustic event recognition, Deep learning, ESR, IEEE\_Xplore},
	pages = {992--1006},
}

@article{Alli2022,
	title = {Data {Augmentation} and {Deep} {Learning} {Methods} in {Sound} {Classification}: {A} {Systematic} {Review}},
	volume = {11},
	issn = {2079-9292},
	doi = {10.3390/electronics11223795},
	abstract = {The aim of this systematic literature review (SLR) is to identify and critically evaluate current research advancements with respect to small data and the use of data augmentation methods to increase the amount of data available for deep learning classifiers for sound (including voice, speech, and related audio signals) classification. Methodology: This SLR was carried out based on the standard SLR guidelines based on PRISMA, and three bibliographic databases were examined, namely, Web of Science, SCOPUS, and IEEE Xplore. Findings. The initial search findings using the variety of keyword combinations in the last five years (2017\&ndash;2021) resulted in a total of 131 papers. To select relevant articles that are within the scope of this study, we adopted some screening exclusion criteria and snowballing (forward and backward snowballing) which resulted in 56 selected articles. Originality: Shortcomings of previous research studies include the lack of sufficient data, weakly labelled data, unbalanced datasets, noisy datasets, poor representations of sound features, and the lack of effective augmentation approach affecting the overall performance of classifiers, which we discuss in this article. Following the analysis of identified articles, we overview the sound datasets, feature extraction methods, data augmentation techniques, and its applications in different areas in the sound classification research problem. Finally, we conclude with the summary of SLR, answers to research questions, and recommendations for the sound classification task.},
	number = {22},
	journal = {Electronics},
	author = {Abayomi-Alli, Olusola O and Damaševičius, Robertas and Qazi, Atika and Adedoyin-Olowe, Mariam and Misra, Sanjay},
	year = {2022},
	keywords = {Data augmentation, Deep learning, Review, SCOPUS},
}

@phdthesis{Rothmund2018,
	address = {Graz, Austria},
	title = {A {Deep} {Learning} {Approach} to {Speech}, {Music} and {Environmental} {Noise} {Classification}},
	school = {University of Music and Performing Arts Graz},
	author = {Rothmund, Felix},
	month = mar,
	year = {2018},
	keywords = {Deep learning, ESR},
}

@article{Piczak2015,
	title = {Environmental sound classification with convolutional neural networks},
	doi = {10.1109/MLSP.2015.7324337},
	journal = {2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP)},
	author = {Piczak, Karol J},
	year = {2015},
	keywords = {CNN, ESR, Semantic\_Scholar},
	pages = {1--6},
}

@article{Salamon2017,
	title = {Deep {Convolutional} {Neural} {Networks} and {Data} {Augmentation} for {Environmental} {Sound} {Classification}},
	volume = {24},
	doi = {10.1109/LSP.2017.2657381},
	number = {3},
	journal = {IEEE Signal Processing Letters},
	author = {Salamon, Justin and Bello, Juan Pablo},
	year = {2017},
	keywords = {CNN, Data augmentation, ESR, IEEE\_Xplore},
	pages = {279--283},
}

@book{Bishop2023,
	address = {Cham, Switzerland},
	edition = {1},
	title = {Deep learning},
	isbn = {978-3-031-45467-7},
	language = {en},
	publisher = {Springer International Publishing},
	author = {Bishop, Christopher M and Bishop, Hugh},
	month = nov,
	year = {2023},
}

@misc{Audacity2024,
	title = {Audacity® ({Version} 3.1.2)},
	url = {https://www.audacityteam.org/},
	author = {{Team Audacity}},
	month = jan,
	year = {2024},
}

@inproceedings{Salamon2014,
	address = {New York, NY, USA},
	title = {A {Dataset} and {Taxonomy} for {Urban} {Sound} {Research}},
	isbn = {978-1-4503-3063-3},
	doi = {10.1145/2647868.2655045},
	booktitle = {Proceedings of the 22nd {ACM} international conference on {Multimedia}},
	publisher = {ACM},
	author = {Salamon, Justin and Jacoby, Christopher and Bello, Juan Pablo},
	month = nov,
	year = {2014},
	keywords = {classification, dataset, environmental sound},
	pages = {1041--1044},
}

@misc{Raspberry2023,
	title = {Raspberry {Pi} 4 {Model} {B}},
	url = {https://www.raspberrypi.com/products/raspberry-pi-4-model-b/},
	urldate = {2024-01-05},
	author = {{Raspberry Pi}},
	month = jan,
	year = {2023},
}

@misc{BURY2024,
	title = {{BURY} {Automotive} ({OEM}) acoustics microphones},
	url = {https://www.bury.com/en/products/microphones/},
	urldate = {2024-01-06},
	author = {{BURY}},
	month = jan,
	year = {2024},
}

@misc{Roadstar2021,
	title = {Roadstar - {The} {Power} of {Sound}},
	url = {http://www.roadstarbrasil.com.br/rs120mic.php},
	urldate = {2024-01-06},
	author = {{Roadstar}},
	month = jan,
	year = {2021},
}

@misc{ReSpeake72,
	title = {{ReSpeaker} {Mic} {Array} v2.0 {Seeed} {Studio} {Wiki}},
	url = {https://wiki.seeedstudio.com/ReSpeaker_Mic_Array_v2.0/},
	urldate = {2024-01-05},
	author = {{Seeed Studio}},
	month = jan,
	year = {2021},
}

@misc{SAE2021,
	title = {Levels of {Driving} {Automation} {Refined} for {Clarity} and {International} {Audience}},
	url = {https://www.sae.org/blog/sae-j3016-update},
	urldate = {2024-01-04},
	author = {{SAE}},
	month = jan,
	year = {2021},
}

@misc{McKinsey2023,
	title = {Autonomous driving’s future: {Convenient} and connected},
	url = {https://www.mckinsey.com/industries/automotive-and-assembly/our-insights/autonomous-drivings-future-convenient-and-connected},
	urldate = {2024-01-04},
	author = {{Deichmann, Johannes} and {Ebel, Eike} and {Heineke, Kersten} and {Heuss, Ruth} and {Kellner, Martin} and {Steiner, Fabian}},
	month = jan,
	year = {2023},
}

@article{Russain2018,
	title = {Autonomous {Cars}: {Research} {Results}, {Issues}, and {Future} {Challenges}},
	volume = {21},
	doi = {10.1109/COMST.2018.2869360},
	number = {2},
	journal = {IEEE Communications Surveys \& Tutorials},
	author = {Hussain, Rasheed and Zeadally, Sherali},
	year = {2019},
	keywords = {IEEE\_Xplore},
	pages = {1275--1313},
}

@book{Wang2006,
	address = {Nashville, TN},
	edition = {1},
	title = {Computational auditory scene analysis: principles, algorithms, and applications},
	isbn = {978-0-471-74109-1},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Wang, Deliang and Brown, Guy J.},
	editor = {Wang, Deliang and Brown, Guy J},
	month = sep,
	year = {2006},
	keywords = {IEEE\_Xplore},
}

@book{Hermes2023,
	address = {Cham, Switzerland},
	edition = {1},
	title = {The perceptual structure of sound},
	isbn = {978-3-031-25565-6},
	language = {en},
	publisher = {Springer International Publishing},
	author = {Hermes, Dik J.},
	month = jun,
	year = {2023},
	note = {Series Title: Current Research in Systematic Musicology},
}

@article{Reddi2018,
	title = {On the {Convergence} of {Adam} and {Beyond}},
	volume = {abs/1904.09237},
	url = {https://api.semanticscholar.org/CorpusID:3455897},
	journal = {ArXiv},
	author = {Reddi, Sashank J and Kale, Satyen and Kumar, Surinder},
	year = {2018},
	keywords = {Semantic\_Scholar},
}

@inproceedings{Hecht-Nielsen1987,
	address = {San Diego, CA},
	title = {Kolmogorov's {Mapping} {Neural} {Network} {Existence} {Theorem}},
	volume = {3},
	url = {https://api.semanticscholar.org/CorpusID:118526925},
	booktitle = {First {IEEE} {International} {Conference} on {Neural} {Networks}},
	publisher = {Piscataway, NJ: IEEE},
	author = {Hecht-Nielsen, Robert},
	year = {1987},
	keywords = {Semantic\_scholar},
	pages = {11--14},
}

@article{mushtaq_spectral_2021,
	title = {Spectral images based environmental sound classification using {CNN} with meaningful data augmentation},
	volume = {172},
	issn = {0003-682X},
	doi = {10.1016/j.apacoust.2020.107581},
	abstract = {In this study, an effective approach of spectral images based on environmental sound classification using Convolutional Neural Networks (CNN) with meaningful data augmentation is proposed. The feature used in this approach is the Mel spectrogram. Our approach is to define features from audio clips in the form of spectrogram images. The randomly selected CNN models used in this experiment are, a 7-layer or a 9-layer CNN learned from scratch. Also, various well-known deep learning structures with transfer learning and with a concept of freezing initial layers, training model, unfreezing the layers, again training the model with discriminative learning are considered. Three datasets, ESC-10, ESC-50, and Us8k are considered. As for the transfer learning methodology, 11 explicit pre-trained deep learning structures are used. In this study, instead of using those available data augmentation schemes for images, we proposed to have meaningful data augmentation by considering variations applied to the audio clips directly. The results show the effectiveness, robustness, and high accuracy of the proposed approach. The meaningful data augmentation can accomplish the highest accuracy with a lower error rate on all datasets by using transfer learning models. Among those used models, The ResNet-152 attained 99.04\% for ESC-10 and 99.49\% for Us8k datasets. DenseNet-161 gained 97.57\% for ESC-50. From our understanding, they are the best-achieved results on these datasets.},
	journal = {Applied Acoustics},
	author = {Mushtaq, Zohaib and Su, Shun-Feng and Tran, Quoc-Viet},
	year = {2021},
	keywords = {Convolutional neural network, Data augmentation, Environment\_Sound\_Classification, Environmental sound classification, Science\_Direct, Spectrogram, Transfer learning},
	pages = {107581},
}

@misc{PiczakESC2015,
	title = {{ESC}: {Dataset} for {Environmental} {Sound} {Classification}},
	doi = {doi/10.7910/DVN/YDEPUT},
	publisher = {Harvard Dataverse},
	author = {Piczak, Karol J.},
	year = {2015},
	keywords = {classification, dataset, environmental sound},
}

@article{Bountourakis2019,
	title = {An {Enhanced} {Temporal} {Feature} {Integration} {Method} for {Environmental} {Sound} {Recognition}},
	volume = {Volume 1, issue 2},
	issn = {2624-599X},
	doi = {10.3390/acoustics1020023},
	language = {English},
	journal = {Acoustics},
	author = {Bountourakis, Vasileios and Vrysis, Lazaros and Konstantoudakis, Konstantinos and Vryzas, Nikolaos},
	year = {2019},
	keywords = {audio classification, classification, dataset, environmental sound, environmental sound recognition, semantic audio analysis, statistical feature integration, temporal feature integration},
	pages = {410--422},
}

@book{Bellman1961,
	address = {Princeton},
	edition = {1},
	title = {Adaptive {Control} {Processes}: {A} {Guided} {Tour}},
	isbn = {978-1-4008-7466-8},
	publisher = {Princeton University Press},
	author = {Bellman, Richard E},
	year = {1961},
	doi = {10.1515/9781400874668},
	keywords = {statistics},
}

@article{Blackmann1958,
	title = {The measurement of power spectra from the point of view of communications engineering — {Part} {I}},
	volume = {37},
	doi = {10.1002/j.1538-7305.1958.tb03874.x},
	number = {1},
	journal = {The Bell System Technical Journal},
	author = {Blackman, R B and Tukey, J W},
	year = {1958},
	keywords = {IEEE\_Xplore},
	pages = {185--282},
}

@inproceedings{Eyben2010,
	address = {New York, NY, USA},
	title = {Opensmile: {The} {Munich} {Versatile} and {Fast} {Open}-{Source} {Audio} {Feature} {Extractor}},
	isbn = {978-1-60558-933-6},
	url = {https://doi.org/10.1145/1873951.1874246},
	doi = {10.1145/1873951.1874246},
	abstract = {We introduce the openSMILE feature extraction toolkit, which unites feature extraction algorithms from the speech processing and the Music Information Retrieval communities. Audio low-level descriptors such as CHROMA and CENS features, loudness, Mel-frequency cepstral coefficients, perceptual linear predictive cepstral coefficients, linear predictive coefficients, line spectral frequencies, fundamental frequency, and formant frequencies are supported. Delta regression and various statistical functionals can be applied to the low-level descriptors. openSMILE is implemented in C++ with no third-party dependencies for the core functionality. It is fast, runs on Unix and Windows platforms, and has a modular, component based architecture which makes extensions via plug-ins easy. It supports on-line incremental processing for all implemented features as well as off-line and batch processing. Numeric compatibility with future versions is ensured by means of unit tests. openSMILE can be downloaded from http://opensmile.sourceforge.net/.},
	booktitle = {Proceedings of the 18th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Eyben, Florian and Wöllmer, Martin and Schuller, Björn},
	year = {2010},
	note = {Series Title: MM '10},
	keywords = {audio feature extraction, emotion, music, signal processing, speech, statistical functionals},
	pages = {1459--1462},
}

@misc{KerasDee32,
	title = {Keras: {Deep} {Learning} for {Humans}},
	url = {https://keras.io/},
	urldate = {2023-12-23},
	author = {{Keras}},
	year = {2023},
}

@misc{scikitle61,
	title = {scikit-learn: {Machine} {Learning} in {Python}},
	url = {https://scikit-learn.org/stable/},
	urldate = {2023-12-23},
	author = {{scikit-learn}},
	year = {2023},
}

@misc{TensorFl23,
	title = {{TensorFlow}},
	url = {https://www.tensorflow.org/},
	urldate = {2023-12-23},
	author = {{Tensorflow}},
	year = {2023},
}

@misc{Anaconda86,
	title = {Anaconda: {The} {World}’s {Most} {Popular} {Data} {Science} {Platform}},
	url = {https://www.anaconda.com/},
	urldate = {2023-12-23},
	author = {{Anaconda}},
	year = {2023},
}

@book{Barber2012,
	address = {Cambridge, England},
	title = {Bayesian reasoning and machine learning},
	isbn = {978-0-521-51814-7},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Barber, David},
	month = feb,
	year = {2012},
}

@book{Alpaydin2014,
	address = {London, England},
	edition = {3},
	title = {Introduction to {Machine} {Learning}},
	isbn = {978-0-262-02818-9},
	language = {en},
	publisher = {MIT Press},
	author = {Alpaydin, Ethem},
	month = aug,
	year = {2014},
	note = {Series Title: Adaptive Computation and Machine Learning series},
}

@book{Bishop2006,
	address = {New York, NY},
	edition = {1},
	title = {Pattern {Recognition} and {Machine} {Learning}},
	isbn = {978-0-387-31073-2},
	language = {en},
	publisher = {Springer},
	author = {Bishop, Christopher M},
	month = aug,
	year = {2006},
	note = {Series Title: Information Science and Statistics},
}

@book{Murphy2012,
	address = {London, England},
	edition = {1},
	title = {Machine {Learning}: {A} {Probabilistic} {Perspective}},
	isbn = {978-0-262-01802-9},
	language = {en},
	publisher = {MIT Press},
	author = {Murphy, Kevin P},
	month = aug,
	year = {2012},
	note = {Series Title: Adaptive Computation and Machine Learning series},
}

@book{Kelleher2020,
	address = {London, England},
	edition = {2},
	title = {Fundamentals of machine learning for predictive data analytics},
	isbn = {978-0-262-04469-1},
	language = {en},
	publisher = {MIT Press},
	author = {Kelleher John D.; Mac Namee, Brian; D'Arcy Aoife},
	month = oct,
	year = {2020},
}

@book{Mitchell1997,
	address = {New York, NY},
	edition = {1},
	title = {Machine {Learning}},
	isbn = {978-0-07-042807-2},
	language = {en},
	publisher = {McGraw-Hill Professional},
	author = {Mitchell, Thomas M},
	month = mar,
	year = {1997},
	note = {Series Title: McGraw-Hill series in computer science},
}

@book{Scholkopf2001,
	address = {London, England},
	edition = {1},
	title = {Learning with kernels},
	isbn = {978-0-262-19475-4},
	language = {en},
	publisher = {MIT Press},
	author = {Scholkopf, Bernhard and Smola, Alexander J.},
	month = dec,
	year = {2001},
	note = {Series Title: Adaptive Computation and Machine Learning series},
}

@book{Russel2010,
	address = {Upper Saddle River, NJ},
	edition = {3},
	title = {Artificial intelligence: {A} {Modern} {Approach}},
	isbn = {978-0-13-604259-4},
	language = {en},
	publisher = {Pearson},
	author = {Russell, Stuart and Norvig, Peter},
	month = dec,
	year = {2010},
	keywords = {artificial\_intelligence},
}

@book{Hastie2009,
	address = {New York, NY},
	edition = {2},
	title = {The elements of {Statistical} {Learning}: {Data} {Mining}, {Inference}, and {Prediction}},
	isbn = {978-0-387-84857-0},
	language = {en},
	publisher = {Springer New York},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2009},
	keywords = {data\_mining, statistics},
}

@misc{McFee2015librosa_sw,
	title = {librosa/librosa: 0.10.1},
	url = {https://doi.org/10.5281/zenodo.8252662},
	publisher = {Zenodo},
	author = {McFee, Brian and McVicar, Matt and Faronbi, Daniel and Roman, Iran and Gover, Matan and Balke, Stefan and Seyfarth, Scott and Malek, Ayoub and Raffel, Colin and Lostanlen, Vincent and van Niekirk, Benjamin and Lee, Dana and Cwitkowitz, Frank and Zalkow, Frank and Nieto, Oriol and Ellis, Dan and Mason, Jack and Lee, Kyungyun and Steers, Bea and Halvachs, Emily and Thomé, Carl and Robert-Stöter, Fabian and Bittner, Rachel and Wei, Ziyao and Weiss, Adam and Battenberg, Eric and Choi, Keunwoo and Yamamoto, Ryuichi and Carr, CJ and Metsai, Alex and Sullivan, Stefan and Friesch, Pius and Krishnakumar, Asmitha and Hidaka, Shunsuke and Kowalik, Steve and Keller, Fabian and Mazur, Dan and Chabot-Leclerc, Alexandre and Hawthorne, Curtis and Ramaprasad, Chandrashekhar and Keum, Myungchul and Gomez, Juanita and Monroe, Will and Morozov, Viktor Andreevitch and Eliasi, Kian and {nullmightybofo} and Biberstein, Paul and Sergin, N. Dorukhan and Hennequin, Romain and Naktinis, Rimvydas and {beantowel} and Kim, Taewoon and Åsen, Jon Petter and Lim, Joon and Malins, Alex and Hereñú, Darío and van der Struijk, Stef and Nickel, Lorenz and Wu, Jackie and Wang, Zhen and Gates, Tim and Vollrath, Matt and Sarroff, Andy and {Xiao-Ming} and Porter, Alastair and Kranzler, Seth and {Voodoohop} and Gangi, Mattia Di and Jinoz, Helmi and Guerrero, Connor and Mazhar, Abduttayyeb and {toddrme2178} and Baratz, Zvi and Kostin, Anton and Zhuang, Xinlu and Lo, Cash TingHin and Campr, Pavel and Semeniuc, Eric and Biswal, Monsij and Moura, Shayenne and Brossier, Paul and Lee, Hojin and Pimenta, Waldir},
	month = aug,
	year = {2023},
	doi = {10.5281/zenodo.8252662},
}

@inproceedings{McFee2015librosa_conf,
	title = {librosa: {Audio} and {Music} {Signal} {Analysis} in {Python}},
	url = {http://dx.doi.org/10.25080/Majora-7b98e3ed-003},
	doi = {10.25080/Majora-7b98e3ed-003},
	urldate = {2023-08-17},
	booktitle = {Proceedings of the 14th {Python} in {Science} {Conference}},
	author = {McFee, Brian and Raffel, Colin and Liang, Dawen and Ellis, Daniel P W and McVicar, Matt and Battenberg, Eric and Nieto, Oriol},
	editor = {Huff, Kathryn and Bergstra, James},
	year = {2015},
	keywords = {Semantic\_Scholar},
	pages = {18 -- 24},
}

@book{Mueller2021,
	title = {Fundamentals of music processing: using {Python} and {Jupyter} notebooks.},
	isbn = {978-3-030-69808-9},
	language = {en},
	publisher = {Springer Nature},
	author = {Mueller, Meinard},
	month = aug,
	year = {2021},
	keywords = {audio, fundamentals},
}

@book{Mueller2016,
	address = {Cham, Switzerland},
	title = {Fundamentals of music processing: audio, analysis, algorithms, applications.},
	isbn = {978-3-319-35765-2},
	language = {en},
	publisher = {Springer International Publishing},
	author = {Mueller, Meinard},
	month = oct,
	year = {2016},
	keywords = {audio, fundamentals},
}

@article{DavisMermelstein1980,
	title = {Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences},
	volume = {28},
	issn = {0096-3518},
	doi = {10.1109/TASSP.1980.1163420},
	number = {4},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Davis, S and Mermelstein, P},
	year = {1980},
	keywords = {Semantic\_Scholar},
	pages = {357--366},
}

@misc{UsingMac94:online,
	title = {Using {Machine} {Learning} in {Safety}-{Critical} {Applications} {Setting} a {Research} {Agenda} {\textbar} {National} {Academies}},
	url = {https://www.nationalacademies.org/our-work/using-machine-learning-in-safety-critical-applications-setting-a-research-agenda},
	urldate = {2023-08-13},
	journal = {National Academies},
	month = feb,
	year = {2023},
}

@book{Barbosa2010,
	address = {Rio de Janeiro, Brasil},
	title = {Interacao {Humano}-computador},
	isbn = {85-352-3418-7},
	publisher = {CAMPUS - GRUPO ELSEVIER},
	author = {Barbosa, Simone Diniz Junqueira and da Silva, Bruno Santana},
	month = dec,
	year = {2010},
	note = {"extra": "bibtex[options=\{useprefix=false\}]"},
	keywords = {PEL214, projeto\_IHC},
}

@inproceedings{Sena2022,
	address = {AV D MANUELL, 27A 2 ESQ, SETUBAL, 2910-595, PORTUGAL},
	title = {{AUDIO}-{MC}: {A} {General} {Framework} for {Multi}-context {Audio} {Classification}},
	isbn = {978-989-758-569-2},
	doi = {10.5220/0011071500003179},
	abstract = {Audio classification is an important research topic in pattern
recognition and has been widely used in several domains, such as
sentiment analysis, speech emotion recognition, environment sound
classification and sound events detection. It consists in predicting a
piece of audio signal into one of the pre-defined semantic classes. In
recent years, researchers have been applied convolution neural networks
to tackle audio pattern recognition problems. However, these approaches
are commonly designed for specific purposes. In this case, machine
learning practitioners, who do not have specialist knowledge in audio
classification, may find it hard to select a proper approach for
different audio contexts. In this paper we propose AUDIO-MC, a general
framework for multi-context audio classification. The main goal of this
work is to ease the adoption of audio classifiers for general machine
learning practitioners, who do not have audio analysis experience.
Experimental results show that our framework achieves better or similar
performance when compared to single-context audio classification
techniques. AUDIO-MC framework shows an accuracy of over 80\% for all
analyzed contexts. In particular, the highest achieved accuracies are
90.60\%, 93.21\% and 98.10\% over RAVDESS, ESC-50 and URBAN datasets,
respectively.},
	language = {English},
	booktitle = {{ICEIS}: {PROCEEDINGS} {OF} {THE} {24TH} {INTERNATIONAL} {CONFERENCE} {ON} {ENTERPRISE} {INFORMATION} {SYSTEMS} - {VOL} 1},
	publisher = {SCITEPRESS},
	author = {Sena, Lucas B. and Praciano, Francisco D. B. S. and Chaves, Iago C. and Brito, Felipe T and Duarte Neto, Eduardo Rodrigues and Monteiro, Jose Maria and Machado, Javam C.},
	editor = {Filipe, J. and Smialek, M. and Brodsky, A and Hammoudi, S.},
	year = {2022},
	keywords = {Audio Classification; Multi-context; Convolutional Neural Networks; Mel
Spectograms, Environment\_Sound\_Classification, Web\_of\_Science},
	pages = {374--383},
}

@inproceedings{WangY2021,
	title = {Environment {Sound} {Classification} ({ESC}) with {Choquet} {Integral} {Fusion}},
	doi = {10.1109/SSCI50451.2021.9660148},
	abstract = {The Choquet integral (ChI) plays an important role in the area of aggregating sensors and information. One of the defining advantages of the ChI, compared to other types of aggregations, is that it takes into account how variables interact with one another, which it does by means of what is called a capacity or fuzzy measure. The fuzzy measure captures the relative value, or worth, of different subsets of information sources taken together to make an inference. For data-driven problems, i.e., machine learning, the fuzzy measure comprises the parameters learned for using the ChI. In this work, we apply data-driven ChI decision-level fusion to the problem of classifying sound events from clips of audio. Three benchmark sound classification data sets are utilized: ESC-10, ESC-50, and UrbanSound8K. Six leading classification algorithms-deep networks and transformers-are used as the decision sources. The ChI fusion shows significant gains in accuracy for all three benchmarks.},
	booktitle = {2021 {IEEE} {Symposium} {Series} on {Computational} {Intelligence} ({SSCI})},
	author = {Wang, Yilin and Havens, Timothy C and Barnard, Andrew},
	month = dec,
	year = {2021},
	keywords = {Environment\_Sound\_Classification, IEEE\_Xplore, Machine learning algorithms;Computational modeling;Machine learning;Computer architecture;Benchmark testing;Transformers;Data models;Decision-level fusion;Choquet integral;vision transformer;CNN;acoustic processing},
	pages = {1--7},
}

@book{WangH2021,
	title = {Environmental {Sound} {Recognition} {Based} on {Residual} {Network} and {Stacking} {Algorithm}},
	volume = {706 LNEE},
	isbn = {9789811584572},
	abstract = {Environmental sound recognition is one of the important tasks in the field of audio research. Because the environment is complex and there is a lot of useless sound information, the traditional methods have low recognition accuracy, which is gradually replaced by related methods of deep learning. In this paper, combined with the latest research in this field, the recognition algorithm based on residual network and stacking method is proposed. The whole is divided into two parts: a feature extractor and a classifier. The residual network is responsible for extracting features with high recognition rate and the stacking algorithm is responsible for accurate recognition. The method is applied to the representative datasets ESC-50 and UrbanSound8k. We obtain a higher accuracy and the model is more clear and simple.},
	author = {Wang, H. and Ren, X. and Zhao, Z.},
	year = {2021},
	doi = {10.1007/978-981-15-8458-9_73},
	note = {Publication Title: Lecture Notes in Electrical Engineering},
	keywords = {Environment\_Sound\_Recognition, SCOPUS},
}

@phdthesis{Nasiri2021a,
	address = {South Caroline},
	title = {Deep {Learning} {Based} {Sound} {Event} {Detection} and {Classification}},
	url = {https://scholarcommons.sc.edu/etd},
	school = {University of South Caroline},
	author = {Nasiri, Alireza},
	year = {2021},
	keywords = {Tese},
}

@techreport{Nasiri2021,
	title = {{SoundCLR}: {Contrastive} {Learning} of {Representations} {For} {Improved} {Environmental} {Sound} {Classification}},
	url = {http://arxiv.org/abs/2103.01929},
	abstract = {Environmental Sound Classification (ESC) is a challenging field of research in non-speech audio processing. Most of current research in ESC focuses on designing deep models with special architectures tailored for specific audio datasets, which usually cannot exploit the intrinsic patterns in the data. However recent studies have surprisingly shown that transfer learning from models trained on ImageNet is a very effective technique in ESC. Herein, we propose SoundCLR, a supervised contrastive learning method for effective environment sound classification with state-of-the-art performance, which works by learning representations that disentangle the samples of each class from those of other classes. Our deep network models are trained by combining a contrastive loss that contributes to a better probability output by the classification layer with a cross-entropy loss on the output of the classifier layer to map the samples to their respective 1-hot encoded labels. Due to the comparatively small sizes of the available environmental sound datasets, we propose and exploit a transfer learning and strong data augmentation pipeline and apply the augmentations on both the sound signals and their log-mel spectrograms before inputting them to the model. Our experiments show that our masking based augmentation technique on the log-mel spectrograms can significantly improve the recognition performance. Our extensive benchmark experiments show that our hybrid deep network models trained with combined contrastive and cross-entropy loss achieved the state-of-the-art performance on three benchmark datasets ESC-10, ESC-50, and US8K with validation accuracies of 99.75{\textbackslash}\%, 93.4{\textbackslash}\%, and 86.49{\textbackslash}\% respectively. The ensemble version of our models also outperforms other top ensemble methods. The code is available at https://github.com/alireza-nasiri/SoundCLR.},
	author = {Nasiri, Alireza and Hu, Jianjun},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.01929},
	keywords = {Artigo, Revisão\_pares},
}

@inproceedings{Sharma2020,
	title = {Environment sound classification using multiple feature channels and attention based deep convolutional neural network},
	volume = {2020-Octob},
	doi = {10.21437/Interspeech.2020-1303},
	abstract = {In this paper, we propose a model for the Environment Sound Classification Task (ESC) that consists of multiple feature channels given as input to a Deep Convolutional Neural Network (CNN) with Attention mechanism. The novelty of the paper lies in using multiple feature channels consisting of Mel-Frequency Cepstral Coefficients (MFCC), Gammatone Frequency Cepstral Coefficients (GFCC), the Constant Q-transform (CQT) and Chromagram. And, we employ a deeper CNN (DCNN) compared to previous models, consisting of spatially separable convolutions working on time and feature domain separately. Alongside, we use attention modules that perform channel and spatial attention together. We use the mix-up data augmentation technique to further boost performance. Our model is able to achieve state-of-the-art performance on three benchmark environment sound classification datasets, i.e. the UrbanSound8K (97.52\%), ESC-10 (94.75\%) and ESC-50 (87.45\%).},
	booktitle = {Proceedings of the {Annual} {Conference} of the {International} {Speech} {Communication} {Association}, {INTERSPEECH}},
	author = {Sharma, J. and Granmo, O.-C. and Goodwin, M.},
	year = {2020},
	keywords = {Environment\_Sound\_Classification, SCOPUS},
	pages = {1186--1190},
}

@inproceedings{Pareta2019,
	title = {Automatic {Environment} {Sounds} {Classification} {Using} {Optimum} {Allocation} {Sampling}},
	isbn = {978-1-72814-740-6},
	doi = {10.1109/ICRAE48301.2019.9043832},
	abstract = {Sound provides highly informative data about the environment. In the sound recognition process, the signal parameterization is an important aspect. In the present work, a new approach using optimum allocation sampling (OAS) method based features used in multi-class least square support vector machine classifier (MC-LS-SVM) is proposed for environmental sound classification (ESC). The time and frequency (TF) features are extracted from the OAS method and these features used as input to MC-LS-SVM classifiers with different kernel functions for automatic ESC. Various performance parameters are computed with Cohen's kappa value being 0.8381 and sensitivity, specificity, F1-score, error and Matthew correlation coefficient are 85.42\%, 98.38\%, 0.854, 14.57\%, 83.81\% respectively. The adaptability and accuracy of the proposed is better as compared to the previously existing methods on the same data-set.},
	booktitle = {2019 4th {International} {Conference} on {Robotics} and {Automation} {Engineering}, {ICRAE} 2019},
	author = {Pareta, A. and Taran, S. and Bajaj, V. and Sengur, A.},
	year = {2019},
	keywords = {Environment\_Sound\_Classification, SCOPUS},
	pages = {69--73},
}

@article{Peng2020,
	title = {Environment {Sound} {Classification} {Based} on {Visual} {Multi}-{Feature} {Fusion} and {GRU}-{AWS}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3032226},
	abstract = {There are two major questions regarding Environmental Sound Classification (ESC). What is the best audio recognition framework, and what is the most robust audio feature? For investigating above problems, the Gated Recurrent Unit (GRU) network was used to analyze the effect of single features such as Mel Scale Spectrogram (Mel), Log-Mel Scale Spectrogram (LM), and Mel frequency cepstral coefficient (MFCC) as well as multi-feature about Mel-MFCC, LM-MFCC, and Mel-LM-MFCC (T-M) in this paper. The experiment results show that in the ESC tasks, multi-features are better than the single features in the same dimensions, and LM-MFCC has the strongest robustness. Meanwhile, reverse sequence MFCC (R-MFCC) and forward and reverse mixed sequence MFCC (FR-MFCC) were also proposed to study the effects of sequence changes on audio. The experiment results show that the sequence transformation of audio features has less influence on the recognition tasks. Furthermore, to investigate the ESC task further we introduced the attention weight similar model (AWS) in to the multi-feature. The AWS model allows different audio feature attention weights of the same sound to learn from each other. It makes the GRU-AWS model focus on the frame-level features more effectively. The experiment results show that the GRU-AWS gets excellent performance with a recognition rate of 94.3\%, and it outperforms other state-of-the-art methods.},
	journal = {IEEE Access},
	author = {Peng, Ning and Chen, Aibin and Zhou, Guoxiong and Chen, Wenjie and Zhang, Wenzhuo and Liu, Jing and Ding, Fubo},
	month = oct,
	year = {2020},
	keywords = {Environment\_Sound\_Classification, Feature extraction;Mel frequency cepstral coefficient;Spectrogram;Time-domain analysis;Task analysis;Hidden Markov models;Neural networks;ESC;GRU;multi-feature fusion;LM-MFCC;sequence transformation;GRU-AWS, IEEE\_Xplore},
	pages = {191100--191114},
}

@inproceedings{Zhao2021,
	title = {Environmental sound classification based on adding noise},
	volume = {2},
	doi = {10.1109/ICIBA52610.2021.9688248},
	abstract = {As ESC is widely used in daily life, it has developed rapidly in recent years. Environment sound classification is a type of sound event recognition (SER). Because of the different position between the sound source and the physical medium of collecting information and the interference of many other sound sources in the process of receiving sound, it leads to the confusion and overlapping of sound events and other complex environmental sound. Combined with the characteristics of environmental sound mentioned above, neural network training with typical experience risk minimization is prone to memory of specific individual voice in the training stage, which will lead to unsatisfactory prediction results when predicting data outside the training distribution, that is, the occurrence of over-fitting. In order to solve the problem of low generalization of neural network, this paper started from the data source to explore the effect of adding gaussian white noise and SNR noise into environmental sound, which are two kinds of audio enhancement algorithms, and organized a series of experiments on Urbansoud8K public environmental sound dataset to verify.},
	booktitle = {2021 {IEEE} 2nd {International} {Conference} on {Information} {Technology}, {Big} {Data} and {Artificial} {Intelligence} ({ICIBA})},
	author = {Zhao, Wen and Yin, Bo},
	month = dec,
	year = {2021},
	keywords = {Environment\_Sound\_Classification, IEEE\_Xplore, Training;Soft sensors;Neural networks;Noise reduction;Interference;White noise;Classification algorithms;environmental sound classification;noise addition;signal-to-noise ratio;neural network},
	pages = {887--892},
}

@inproceedings{Tripathi2022,
	title = {Revamped {Knowledge} {Distillation} for {Sound} {Classification}},
	doi = {10.1109/IJCNN55064.2022.9892474},
	abstract = {This paper presents a novel knowledge distillation technique that inherits knowledge from multiple deep Environment Sound Classification (ESC) models trained on spectrogram features created by dividing the spectrogram into multiple subband spectrogram. The deep models trained on sub-band spectrograms prevent information loss while performing knowledge distillation from a teacher model to a student model receiving the full spectrogram as an input. The student models' performance is evaluated on two benchmark sound datasets, viz. the ESC-10 and Audio MNIST datasets. The impact of teacher models trained with different number of sub-band features and four ensemble techniques has been investigated thoroughly to enhance the final accuracy of the student model supervised by the proposed knowledge distillation framework. Experiments and results shows that the accuracy of the student model is comparable and competitive to state-of-the-art methods for sound classification. Moreover, the student model trained on the Audio MNIST dataset attains an hitherto unpublished accuracy of 98.25\%, a new benchmark for the Audio MNIST dataset. Additionally, Grad- CAM visualization of the spectrogram features is generated to identify the spectrogram's relevant regions and understand why the model classifies a signal into a specific class.},
	booktitle = {2022 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Tripathi, Achyut Mani and Mishra, Aakansha},
	month = jul,
	year = {2022},
	note = {ISSN: 2161-4407},
	keywords = {Environment\_Sound\_Classification, IEEE\_Xplore, Knowledge engineering;Visualization;Neural networks;Benchmark testing;Spectrogram},
	pages = {1--8},
}

@inproceedings{Olteanu2019,
	title = {Fusion of speech techniques for automatic environmental sound recognition},
	doi = {10.1109/SPED.2019.8906574},
	abstract = {Environmental sound recognition is currently an important field of computer science and robotics, and a useful component in content-based search, security or environmental protection, framed in a new discipline called Automatic Environment Sound Recognition. We will evaluate how suitable is speech specific technology to be applied to environmental sound recognition, and how the performance can be grown by extending the feature space or by fusion of several sound technologies results. The problem addresses an environmental issue: preventing deforestation by detecting specific acoustic signal such as chainsaw, in a forest environment. The experiments consider three classes of forest specific sounds and are meant to detect the chainsaw sounds.},
	booktitle = {2019 {International} {Conference} on {Speech} {Technology} and {Human}-{Computer} {Dialogue} ({SpeD})},
	author = {Olteanu, E and Miu, D O and Drosu, A and Segarceanu, S and Suciu, G and Gavat, I},
	year = {2019},
	keywords = {AESR, DTW, Environment\_Sound\_Recognition, IEEE\_Xplore, MFCC –GMM, fusion of results, speech technology},
	pages = {1--8},
}

@inproceedings{Ebbers2021,
	title = {Adapting {Sound} {Recognition} to {A} {New} {Environment} {Via} {Self}-{Training}},
	isbn = {2076-1465},
	doi = {10.23919/EUSIPCO54536.2021.9616009},
	abstract = {Recently, there has been a rising interest in sound recognition via acoustic sensor networks (ASNs) to support applications such as ambient assisted living or environmental habitat monitoring. With state-of-the-art sound recognition being dominated by deep-learning-based approaches, there is a high demand for labeled training data. Despite the availability of large-scale data sets such as Google's AudioSet, acquiring training data matching a certain application environment is still often a problem. In this paper we are concerned with human activity monitoring in a domestic environment using an ASN consisting of multiple nodes each providing multichannel signals. We propose a self-training based domain adaptation approach, which only requires unlabeled data from the target environment. Here, a sound recognition system trained on AudioSet, the teacher, generates pseudo labels for data from the target environment on which a student network is trained. The student can furthermore glean information about the spatial arrangement of sensors and sound sources to further improve classification performance. It is shown that the student significantly improves recognition performance over the pre-trained teacher without relying on labeled data from the environment the system is deployed in.},
	booktitle = {2021 29th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	author = {Ebbers, J and Keyser, M C and Haeb-Umbach, R},
	year = {2021},
	keywords = {Acoustic sensors, Ambient assisted living, Environment\_Sound\_Recognition, Europe, IEEE\_Xplore, Sensor phenomena and characterization, Signal processing, Target recognition, Training data, acoustic sensor network, domain adaptation, scene classification, self-training, sound recognition},
	pages = {1135--1139},
}

@inproceedings{Chen2021,
	title = {Two-stream convolutional neural networks based on a self-attention mechanism for environmental sound classification},
	volume = {5},
	isbn = {2693-3128},
	doi = {10.1109/ITNEC52019.2021.9587213},
	abstract = {In recent years, with the development of smart homes, urban sound monitoring and machine hearing, the accurate recognition of environmental sound has become particularly important. However, due to the suddenness of environmental sound and the tendency to mix with other noises, the accuracy of environmental sound recognition has been unsatisfactory. In order to improve the recognition accuracy of environmental sounds, this paper designs a decision layer fusion module (SAD) based on self-attention mechanism, and introduces the self-attention mechanism into the decision layer part of the two-stream convolutional neural network. We also constructed the SAD-CNN model architecture to better fuse the decision layer outputs of convolutional neural networks with different inputs to improve the accuracy of environment sound recognition. We investigated the performance of different fusion methods based on the UrbanSound8K dataset, and the experimental results validated the effectiveness of the SAD module and the SAD-CNN model architecture, with the best recognition accuracy of 96.24\% for the UrbanSound8K dataset},
	booktitle = {2021 {IEEE} 5th {Information} {Technology},{Networking},{Electronic} and {Automation} {Control} {Conference} ({ITNEC})},
	author = {Chen, Z and Yin, B},
	year = {2021},
	keywords = {Conferences, Convolutional neural networks, Data models, Environment\_Sound\_Recognition, Environmental sound classification, Fuses, IEEE\_Xplore, Network architecture, Neural networks, Smart homes, convolutional neural network, data augmentation, decision-level fusion, self-attention mechanism},
	pages = {1047--1051},
}

@article{Tripath2021,
	title = {Environment sound classification using an attention-based residual neural network},
	volume = {460},
	issn = {0925-2312},
	doi = {10.1016/j.neucom.2021.06.031},
	abstract = {Complexity of environmental sounds impose numerous challenges for their
classification. The performance of Environmental Sound Classification
(ESC) depends greatly on how good the feature extraction technique
employed to extract generic and prototypical features from a sound is.
The presence of silent and semantically irrelevant frames is ubiquitous
during the classification of environmental sounds. To deal with such
issues that persist in environmental sound classification, we introduce
a novel attention-based deep model that supports focusing on
semantically relevant frames. The proposed attention guided deep model
efficiently learns spatio-temporal relationships that exist in the
spectrogram of a signal. The efficacy of the proposed method is
evaluated on two widely used Environmental Sound Classification
datasets: ESC-10 and DCASE 2019 Task-1(A) datasets. The experiments
performed and their results demonstrate that the proposed method yields
comparable performance to state-of-the-art techniques. We obtained
improvements of 11.50\% and 19.50\% in accuracy as compared to the
accuracy of the baseline models of the ESC-10 and DCASE 2019 Task-1(A)
datasets respectively. To support the attention outcomes that have
focused on relevant regions, visual analysis of the attention feature
map has also been presented. The resultant attention feature map conveys
that the model focuses only on the spectrogram's semantically relevant
regions while skipping the irrelevant regions. (c) 2021 Elsevier B.V.
All rights reserved.},
	language = {English},
	journal = {NEUROCOMPUTING},
	author = {Tripathi, Achyut Mani and Mishra, Aakansha},
	month = oct,
	year = {2021},
	note = {Publisher: ELSEVIER
Place: RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
	keywords = {Attention mechanism; Convolutional neural network; Explainable;
Environmental sound classification; Residual network, Environment\_Sound\_Classification, Web\_of\_Science},
	pages = {409--423},
}

@article{Fan2020,
	title = {Deep neural network based environment sound classification and its implementation on hearing aid app},
	volume = {159},
	issn = {0263-2241},
	doi = {10.1016/j.measurement.2020.107790},
	abstract = {In general, a hearing aid app is very useful for the persons having
either partial or complete inability to hear. At present, there is no
special provision available in the hearing aid app for the
classification of different environmental sounds. This paper proposes an
algorithm for environmental sound classification based on Superimposed
Audio Blocks using Deep Neural Networks (SAB - DNN) and also to
implement it on the hearing aid app. The system can recognize five kinds
of different sound fields automatically: bus, subway, street, indoor,
car. In this system, 512 sampling points are taken as an audio frame and
several audio frames are stacked up into an Audio Block (AB). when 7
audio frames are stacked up into an Audio Block (AB), the accuracy rate
of sound environment classification using (AB - DNN) tends to be the
best (96.18\%). Based on this, the experiment integrates multiple Audio
Block (AB) into an audio unit called Superimposed Audio Blocks(SAB) and
classify it using DNN. Optimally, 30 sound blocks are integrated into a
SAB which results in the classification accuracy up to 98.8\%. As far as
we know, it is the first time on the hearing aid app to implement an
improved Deep Neural Network (DNN) based classification system and
superposition of multi-audio frames and blocks. (C) 2020 Elsevier Ltd.
All rights reserved.},
	language = {English},
	journal = {MEASUREMENT},
	author = {Fan, Xiaoqian and Sun, Tianyi and Chen, Wenzhi and Fan, Quanfang},
	month = jul,
	year = {2020},
	note = {Publisher: ELSEVIER SCI LTD
Place: THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
	keywords = {Environment\_Sound\_Classification, Sound environment classification; Superimposed audio blocks(SAB); Deep
neural networks; Feature extraction; Hearing aids, Web\_of\_Science},
}

@inproceedings{Patil2019,
	address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
	title = {Content-{Based} {Audio} {Classification} and {Retrieval} {Using} {Segmentation}, {Feature} {Extraction} and {Neural} {Network} {Approach}},
	volume = {924},
	isbn = {978-981-13-6861-5; 978-981-13-6860-8},
	doi = {10.1007/978-981-13-6861-5_23},
	abstract = {The volume of audio data is increasing tremendously daily on public
networks like Internet. This increases the difficulty in accessing those
audio data. Hence, there is a need of efficient indexing and annotation
mechanisms. Non-stationarity and discontinuity present in the audio
signal rise the difficulty in segmentation and classification of audio
signals. The other challenging task is to extract and select the optimal
features in audio signal. The application areas of audio classification
and retrieval system include speaker recognition, gender classification,
music genre classification, environment sound classification, etc. This
paper proposes a machine learning- and neural network-based approach
which performs audio pre-processing, segmentation, feature extraction,
classification and retrieval of audio signal from the dataset. We have
proposed novel approach of classification and retrieval using FPNN by
combining fuzzy logic and PNN characteristics. We found that FPNN
classifier gives better accuracy, F1-score and Kappa coefficient values
compared to SVM, k-NN and PNN classifiers.},
	language = {English},
	booktitle = {{ADVANCES} {IN} {COMPUTER} {COMMUNICATION} {AND} {COMPUTATIONAL} {SCIENCES}, {IC4S} 2018},
	publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
	author = {Patil, Nilesh M and Nemade, Milind U},
	editor = {Bhatia, S K and Tiwari, S and Mishra, K K and Trivedi, M C},
	year = {2019},
	note = {Series Title: Advances in Intelligent Systems and Computing
ISSN: 2194-5357},
	keywords = {Environment\_Sound\_Classification, SVM; k-NN; PNN; FPNN; Accuracy; Recall; Precision, Web\_of\_Science},
	pages = {263--281},
}

@article{Zhang2022,
	title = {Sound classification using evolving ensemble models and {Particle} {Swarm} {Optimization}},
	volume = {116},
	issn = {15684946},
	doi = {10.1016/j.asoc.2021.108322},
	abstract = {Automatic sound classification attracts increasing research attention owing to its vast applications, such as robot navigation, environmental sensing, musical instrument classification, medical diagnosis, and surveillance. In this research, we propose an ensemble convolutional bidirectional Long Short-Term Memory (CBiLSTM) network with optimal hyper-parameter selection for undertaking sound classification. We first transform each audio signal into a spectrogram representation using the Short-time Fourier transform (STFT). A Particle Swarm Optimization (PSO) variant is subsequently proposed to optimize the learning rate, weight decay, numbers of filters and hidden units in the convolutional and BiLSTM layers, respectively, in order to extract effective spatial–temporal characteristics from the spectrogram inputs. To tackle the issue of stagnation in optimization, the proposed algorithm incorporates local exploitation using secant and Newton–Raphson methods, promising leader generation using regular and irregular super-ellipse formulae, and three-dimensional spherical search coefficients. Moreover, it takes into account multiple fused elite signals in conjunction with numerical analysis based exploitation to balance between diversification and intensification. A variety of CBiLSTM networks with distinctive optimized settings are devised. An ensemble model is then constructed by incorporating a set of three yielded networks based on a majority voting scheme. Evaluated using several audio data sets, our ensemble CBiLSTM networks outperform those with default and optimal settings identified by other search methods, existing deep architectures and state-of-the-art related studies. In addition to sound classification tasks, the proposed PSO algorithm also outperforms a number of classical and advanced search methods for solving diverse unimodal and multimodal benchmark functions with statistical significance.},
	journal = {Applied Soft Computing},
	author = {Zhang, Li and Lim, Chee Peng and Yu, Yonghong and Jiang, Ming},
	month = feb,
	year = {2022},
	note = {Publisher: Elsevier Ltd},
	keywords = {Artigo, Deep convolutional bidirectional long short-term memory network and ensemble classifier, Evolutionary algorithm, Periódico, Sound classification},
}

@article{Turchet2020,
	title = {The {Internet} of {Audio} {Things}: {State} of the {Art}, {Vision}, and {Challenges}},
	volume = {7},
	issn = {23274662},
	doi = {10.1109/JIOT.2020.2997047},
	abstract = {The Internet of Audio Things (IoAuT) is an emerging research field positioned at the intersection of the Internet of Things, sound and music computing, artificial intelligence, and human-computer interaction. The IoAuT refers to the networks of computing devices embedded in physical objects (Audio Things) dedicated to the production, reception, analysis, and understanding of audio in distributed environments. Audio Things, such as nodes of wireless acoustic sensor networks, are connected by an infrastructure that enables multidirectional communication, both locally and remotely. In this article, we first review the state of the art of this field, then we present a vision for the IoAuT and its motivations. In the proposed vision, the IoAuT enables the connection of digital and physical domains by means of appropriate information and communication technologies, fostering novel applications and services based on auditory information. The ecosystems associated with the IoAuT include interoperable devices and services that connect humans and machines to support human-human and human-machines interactions. We discuss the challenges and implications of this field, which lead to future research directions on the topics of privacy, security, design of Audio Things, and methods for the analysis and representation of audio-related information.},
	number = {10},
	journal = {IEEE Internet of Things Journal},
	author = {Turchet, Luca and Fazekas, Gyorgy and Lagrange, Mathieu and Ghadikolaei, Hossein S. and Fischione, Carlo},
	month = oct,
	year = {2020},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Artigo, Auditory scene analysis, Internet of Audio Things (IoAuT), Internet of Sounds, Periódico, ecoacoustics, smart city},
	pages = {10233--10249},
}

@article{Ragab2021,
	title = {An ensemble one dimensional convolutional neural network with bayesian optimization for environmental sound classification},
	volume = {11},
	issn = {20763417},
	doi = {10.3390/app11104660},
	abstract = {With the growth of deep learning in various classification problems, many researchers have used deep learning methods in environmental sound classification tasks. This paper introduces an end-to-end method for environmental sound classification based on a one-dimensional convolution neural network with Bayesian optimization and ensemble learning, which directly learns features representation from the audio signal. Several convolutional layers were used to capture the signal and learn various filters relevant to the classification problem. Our proposed method can deal with any audio signal length, as a sliding window divides the signal into overlapped frames. Bayesian optimization accomplished hyperparameter selection and model evaluation with cross-validation. Multiple models with different settings have been developed based on Bayesian optimization to ensure network convergence in both convex and non-convex optimization. An UrbanSound8K dataset was evaluated for the performance of the proposed end-to-end model. The experimental results achieved a classification accuracy of 94.46\%, which is 5\% higher than existing end-to-end approaches with fewer trainable parameters. Four measurement indices, namely: sensitivity, specificity, accuracy, precision, recall, F-measure, area under ROC curve, and the area under the precision-recall curve were used to measure the model performance. The proposed approach outperformed state-of-the-art end-to-end approaches that use hand-crafted features as input in selected measurement indices and time complexity.},
	number = {10},
	journal = {Applied Sciences (Switzerland)},
	author = {Ragab, Mohammed Gamal and Abdulkadir, Said Jadid and Aziz, Norshakirah and Alhussian, Hitham and Bala, Abubakar and Alqushaibi, Alawi},
	month = may,
	year = {2021},
	note = {Publisher: MDPI AG},
	keywords = {Artigo, Bayesian optimization, Convolutional neural networks, Deep learning, Ensemble learning, Environmental sound classification, Optimization, Periódico, UrbanSound8k},
}

@article{Nanni2021,
	title = {An ensemble of convolutional neural networks for audio classification},
	volume = {11},
	issn = {20763417},
	doi = {10.3390/app11135796},
	abstract = {Research in sound classification and recognition is rapidly advancing in the field of pattern recognition. One important area in this field is environmental sound recognition, whether it concerns the identification of endangered species in different habitats or the type of interfering noise in urban environments. Since environmental audio datasets are often limited in size, a robust model able to perform well across different datasets is of strong research interest. In this paper, ensembles of classifiers are combined that exploit six data augmentation techniques and four signal representations for retraining five pre-trained convolutional neural networks (CNNs); these ensembles are tested on three freely available environmental audio benchmark datasets: (i) bird calls, (ii) cat sounds, and (iii) the Environmental Sound Classification (ESC-50) database for identifying sources of noise in environments. To the best of our knowledge, this is the most extensive study investigating ensembles of CNNs for audio classification. The best-performing ensembles are compared and shown to either outperform or perform comparatively to the best methods reported in the literature on these datasets, including on the challenging ESC-50 dataset. We obtained a 97\% accuracy on the bird dataset, 90.51\% on the cat dataset, and 88.65\% on ESC-50 using different approaches. In addition, the same ensemble model trained on the three datasets managed to reach the same results on the bird and cat datasets while losing only 0.1\% on ESC-50. Thus, we have managed to create an off-the-shelf ensemble that can be trained on different datasets and reach performances competitive with the state of the art.},
	number = {13},
	journal = {Applied Sciences (Switzerland)},
	author = {Nanni, Loris and Maguolo, Gianluca and Brahnam, Sheryl and Paci, Michelangelo},
	month = jul,
	year = {2021},
	note = {arXiv: 2007.07966
Publisher: MDPI AG},
	keywords = {Artigo, Audio classification, Data augmentation, Ensemble of classifiers, Pattern recognition, Periódico},
}

@article{Mushtaq2020,
	title = {Efficient classification of environmental sounds through multiple features aggregation and data enhancement techniques for spectrogram images},
	volume = {12},
	issn = {20738994},
	doi = {10.3390/sym12111822},
	abstract = {Over the past few years, the study of environmental sound classification (ESC) has become very popular due to the intricate nature of environmental sounds. This paper reports our study on employing various acoustic features aggregation and data enhancement approaches for the effective classification of environmental sounds. The proposed data augmentation techniques are mixtures of the reinforcement, aggregation, and combination of distinct acoustics features. These features are known as spectrogram image features (SIFs) and retrieved by different audio feature extraction techniques. All audio features used in this manuscript are categorized into two groups: one with general features and the other with Mel filter bank-based acoustic features. Two novel and innovative features based on the logarithmic scale of the Mel spectrogram (Mel), Log (Log-Mel) and Log (Log (Log-Mel)) denoted as L2M and L3M are introduced in this paper. In our study, three prevailing ESC benchmark datasets, ESC-10, ESC-50, and Urbansound8k (Us8k) are used. Most of the audio clips in these datasets are not fully acquired with sound and include silence parts. Therefore, silence trimming is implemented as one of the pre-processing techniques. The training is conducted by using the transfer learning model DenseNet-161, which is further fine-tuned with individual optimal learning rates based on the discriminative learning technique. The proposed methodologies attain state-of-the-art outcomes for all used ESC datasets, i.e., 99.22\% for ESC-10, 98.52\% for ESC-50, and 97.98\% for Us8k. This work also considers real-time audio data to evaluate the performance and efficiency of the proposed techniques. The implemented approaches also have competitive results on real-time audio data.},
	number = {11},
	journal = {Symmetry},
	author = {Mushtaq, Zohaib and Su, Shun Feng},
	month = nov,
	year = {2020},
	note = {Publisher: MDPI AG},
	keywords = {Artigo, Data augmentation, ESC-10, ESC-50, Environmental sound classification, Features aggregation, Periódico, Transfer learning, Urban sound 8k},
	pages = {1--34},
}

@article{Mu2021,
	title = {Environmental sound classification using temporal-frequency attention based convolutional neural network},
	volume = {11},
	issn = {20452322},
	doi = {10.1038/s41598-021-01045-4},
	abstract = {Environmental sound classification is one of the important issues in the audio recognition field. Compared with structured sounds such as speech and music, the time–frequency structure of environmental sounds is more complicated. In order to learn time and frequency features from Log-Mel spectrogram more effectively, a temporal-frequency attention based convolutional neural network model (TFCNN) is proposed in this paper. Firstly, an experiment that is used as motivation in proposed method is designed to verify the effect of a specific frequency band in the spectrogram on model classification. Secondly, two new attention mechanisms, temporal attention mechanism and frequency attention mechanism, are proposed. These mechanisms can focus on key frequency bands and semantic related time frames on the spectrogram to reduce the influence of background noise and irrelevant frequency bands. Then, a feature information complementarity is formed by combining these mechanisms to more accurately capture the critical time–frequency features. In such a way, the representation ability of the network model can be greatly improved. Finally, experiments on two public data sets, UrbanSound 8 K and ESC-50, demonstrate the effectiveness of the proposed method.},
	number = {1},
	journal = {Scientific Reports},
	author = {Mu, Wenjie and Yin, Bo and Huang, Xianqing and Xu, Jiali and Du, Zehua},
	month = dec,
	year = {2021},
	pmid = {34732762},
	note = {Publisher: Nature Research},
	keywords = {Artigo, Periódico},
}

@article{Liu2021,
	title = {Environmental {Sound} {Classification} {Based} on {Stacked} {Concatenated} {DNN} using {Aggregated} {Features}},
	volume = {93},
	issn = {19398115},
	doi = {10.1007/s11265-021-01702-x},
	abstract = {In recent years, there has been an increasing interest in Environmental Sound Classification (ESC), and it is a challenging non-speech audio event classification problem because of the complexity of the environment. However, the classification accuracy of the conventional methods is significantly dependent on the robustness of representative features and the effectiveness of the constructed model, which causes the poor adaptability of current models. Considering this, a novel ESC scheme based on stacked Deep Neural Networks with multi-dimensional aggregated features is proposed. Firstly, we use the aggregated features composed of time-domain features and time–frequency (TF) domain features to capture a more comprehensive representation of sounds. Afterward, the feature reduction based on Principal Component Analysis (PCA) is employed to select the most discriminative representations. Finally, a novel Stacked Deep Neural Networks based on ensemble learning and data augmentation is presented to improve the ESC scheme's generalizing capability. The experimental results demonstrate that the proposed method is appropriate for ESC problems, which achieves 96.1\% and 98.1\% accuracy scores for ESC-10 and UrbanSound8K datasets, respectively, and outperforms most state-of-art methods in ESC tasks at the aspect of both accuracy and computational burden.},
	number = {11},
	journal = {Journal of Signal Processing Systems},
	author = {Liu, Chengwei and Hong, Feng and Feng, Haihong and Zhai, Yushuang and Chen, Youyuan},
	month = nov,
	year = {2021},
	note = {Publisher: Springer},
	keywords = {Artigo, Auditory Feature, Deep Neural Networks, Environmental Sound Classification, Feature Aggregation, Periódico},
	pages = {1287--1299},
}

@article{Farahani2022,
	title = {A spatially based machine learning algorithm for potential mapping of the hearing senses in an urban environment},
	volume = {80},
	issn = {22106707},
	doi = {10.1016/j.scs.2022.103675},
	abstract = {Mapping individuals’ sense of hearing in the urban environment helps urban managers and planners accomplish goals such as creating a favorable urban environment for the citizens. The present study has been conducted to address the lack of modeling and compilation of a sense of hearing potential map in the urban environment and can help urban managers and planners make decisions in this regard. The present study aims for spatial modeling of people's hearing senses and developing potential maps for various hearing states in Tehran, Iran, using a random forest (RF) machine learning algorithm. The four various states, including pleasant sound, annoying sound, normal sound, and stressful sound, have been considered in the present study for the sounds that can be heard in the city. First, a spatial database made up of dependent, and independent data was built. Dependent data included people's four states of hearing in the urban environment, and the respective data was collected through a questionnaire from 657 people. Independent data were categorized into four groups. The first group was traffic related noises including the criteria of traffic volume and equivalent continuous sound level (Leq), the second group included land use related criteria of distance to cemetery, distance to sports areas, distance to commercial areas, distance to primary streets, distance to secondary streets, distance to park, and distance to industrial areas. Furthermore, the public facilities related group includes the distance to airports and distance to public transportation stations, and the population related group includes population density. 70\% of the data were used for training, and 30\% were set aside for validation. The spatial database was used to develop the potential map for the four states of the hearing sense in the urban environment using the RF algorithm. The potential hearing sense map was evaluated using the receiver operating characteristic (ROC) curve and the respective area under the curve (AUC). The AUC values were 0.930, 0.957, 0.950, and 0.903 for each of the pleasant, annoying, normal, and stressful states, respectively. There was a higher potential for pleasant sounds in the northern districts and some of the central ones, for annoying sounds mainly in the central districts, for normal sounds in central and southern districts, and stressful sounds in some parts of the central and mainly southern districts.},
	journal = {Sustainable Cities and Society},
	author = {Farahani, Mahsa and Razavi-Termeh, Seyed Vahid and Sadeghi-Niaraki, Abolghasem},
	month = may,
	year = {2022},
	note = {Publisher: Elsevier Ltd},
	keywords = {Artigo, Machine learning, Periódico, Sense of hearing, Spatial modeling, Urban environment},
}

@article{Brambilla2020,
	title = {Classification of urban road traffic noise based on sound energy and eventfulness indicators},
	volume = {10},
	issn = {20763417},
	doi = {10.3390/app10072451},
	abstract = {Noise energetic indicators, like Lden, show good correlations with long term annoyance, but should be supplemented by other parameters describing the sound fluctuations, which are very common in urban areas and negatively impact noise annoyance. Thus, in this paper, the hourly values of continuous equivalent level LAeqh and the intermittency ratio (IR) were both considered to describe the urban road traffic noise, monitored in 90 sites in the city of Milan and covering different types of road, from motorways to local roads. The noise data have been processed by clustering methods to detect similarities and to figure out a criterion to classify the urban sites taking into account both equivalent noise levels and road traffic noise events. Two clusters were obtained and, considering the cluster membership of each site, the decimal logarithm of the day-time (06:00-22:00) traffic flow was used to associate each new road with the clusters. In particular, roads with average day-time hourly traffic flow ≥1900 vehicles/hour were associated with the cluster with high traffic flow. The described methodology could be fruitfully applied on road traffic noise data in other cities.},
	number = {7},
	journal = {Applied Sciences (Switzerland)},
	author = {Brambilla, Giovanni and Benocci, Roberto and Confalonieri, Chiara and Roman, Hector Eduardo and Zambon, Giovanni},
	month = apr,
	year = {2020},
	note = {Publisher: MDPI AG},
	keywords = {Artigo, Noise events, Periódico, Road classification, Urban road traffic noise},
}

@article{Aziz2019,
	title = {Automatic scene recognition through acoustic classification for behavioral robotics},
	volume = {8},
	issn = {20799292},
	doi = {10.3390/electronics8050483},
	abstract = {Classification of complex acoustic scenes under real time scenarios is an active domain which has engaged several researchers lately form the machine learning community. A variety of techniques have been proposed for acoustic patterns or scene classification including natural soundscapes such as rain/thunder, and urban soundscapes such as restaurants/streets, etc. In this work, we present a framework for automatic acoustic classification for behavioral robotics. Motivated by several texture classification algorithms used in computer vision, a modified feature descriptor for sound is proposed which incorporates a combination of 1-D local ternary patterns (1D-LTP) and baseline method Mel-frequency cepstral coefficients (MFCC). The extracted feature vector is later classified using a multi-class support vector machine (SVM), which is selected as a base classifier. The proposed method is validated on two standard benchmark datasets i.e., DCASE and RWCP and achieves accuracies of 97.38\% and 94.10\%, respectively. A comparative analysis demonstrates that the proposed scheme performs exceptionally well compared to other feature descriptors.},
	number = {5},
	journal = {Electronics (Switzerland)},
	author = {Aziz, Sumair and Awais, Muhammad and Akram, Tallha and Khan, Umar and Alhussein, Musaed and Aurangzeb, Khursheed},
	month = may,
	year = {2019},
	note = {Publisher: MDPI AG},
	keywords = {Artigo, Feature extraction, MFCC, Periódico, Robotics, Sound classification, Sound processing, Support vector machine},
}

@article{Ahmed2020,
	title = {Automatic {Environmental} {Sound} {Recognition} ({AESR}) using convolutional neural network},
	volume = {12},
	issn = {2075017X},
	doi = {10.5815/ijmecs.2020.05.04},
	abstract = {Automatic Environmental Sound Recognition (AESR) is an essential topic in modern research in the field of pattern recognition. We can convert a short audio file of a sound event into a spectrogram image and feed that image to the Convolutional Neural Network (CNN) for processing. Features generated from that image are used for the classification of various environmental sound events such as sea waves, fire cracking, dog barking, lightning, raining, and many more. We have used the log-mel spectrogram auditory feature for training our six-layer stack CNN model. We evaluated the accuracy of our model for classifying the environmental sounds in three publicly available datasets and achieved an accuracy of 92.9\% in the urbansound8k dataset, 91.7\% accuracy in the ESC-10 dataset, and 65.8\% accuracy in the ESC-50 dataset. These results show remarkable improvement in precise environmental sound recognition using only stack CNN compared to multiple previous works, and also show the efficiency of the log-mel spectrogram feature in sound recognition compared to Mel Frequency Cepstral Coefficients (MFCC), Wavelet Transformation, and raw waveform. We have also experimented with the newly published Rectified Adam (RAdam) as the optimizer. Our study also shows a comparative analysis between the Adaptive Learning Rate Optimizer (Adam) and RAdam optimizer used in training the model to correctly classifying the environmental sounds from image recognition architecture.},
	number = {5},
	journal = {International Journal of Modern Education and Computer Science},
	author = {Ahmed, Md Rayhan and Robin, Towhidul Islam and Shafin, Ashfaq Ali},
	year = {2020},
	note = {Publisher: Modern Education and Computer Science Press},
	keywords = {AESR, Adam, CNN, Classification, Image, Log-Mel Spectrogram, MFCC, RAdam, Relu, artigo, periodico},
	pages = {41--54},
}

@inproceedings{Seker2020,
	title = {{CnnSound}: {Convolutional} {Neural} {Networks} for the {Classification} of {Environmental} {Sounds}},
	isbn = {978-1-4503-8784-2},
	doi = {10.1145/3441417.3441431},
	abstract = {The classification of environmental sounds (ESC) has been increasingly studied in recent years. The main reason is that environmental sounds are part of our daily life, and associating them with our environment that we live in is important in several aspects as ESC is used in areas such as managing smart cities, determining location from environmental sounds, surveillance systems, machine hearing, environment monitoring. The ESC is however more difficult than other sounds because there are too many parameters that generate background noise in the ESC, which makes the sound more difficult to model and classify. The main aim of this study is therefore to develop more robust convolution neural networks architecture (CNN). For this purpose, 150 different CNN-based models were designed by changing the number of layers and values of their tuning parameters used in the layers. In order to test the accuracy of the models, the Urbansound8k environmental sound database was used. The sounds in this data set were first converted into an image format of 32x32x3. The proposed CNN model has yielded an accuracy of as much as 82.5\% being higher than its classical counterpart. As there was not that much fine-tuning, the obtained accuracy has been found to be better and satisfactory compared to other studies on the Urbansound8k when both accuracy and computational complexity are considered. The results also suggest further improvement possible due to low complexity of the proposed CNN architecture and its applicability in real-world settings.},
	booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
	publisher = {Association for Computing Machinery},
	author = {Seker, Huseyin and Inik, Ozkan},
	month = oct,
	year = {2020},
	keywords = {Artigo, Conferência, Convolutional Neural Networks (CNN), Deep Learning, Environmental sound classification (ESC), Urbansound8k},
	pages = {79--84},
}

@inproceedings{Niranjan2021,
	title = {Ensemble and {Multi} {Model} approach to {Environmental} {Sound} {Classification}},
	isbn = {978-1-66541-480-7},
	doi = {10.1109/ICECCT52121.2021.9616775},
	abstract = {AI plays an important role in acoustics recognition. Importantly, being able to automatically and accurately identify environmental sounds opens up a broad range of applications. Deep learning techniques can assist in the recognition of sounds which we come across in our day-to-day life. Most of the previous work in environmental sound classification involves training a model on a single set of features. In our work, we extract two sets of features from the audio namely Mel spectrograms and Mel Frequency Cepstral Coefficient Spectrograms. We propose two CNN-based approaches through the usage of Ensemble of models which are trained on Mel Spectrograms and a multi model network which is trained on both Mel and MFCC Spectrograms. The proposed approaches were trained and tested on the ESC-50 dataset which contains 2000 samples of audio recordings distributed into 50 categories. Performance evaluation revealed that both our proposed approaches achieve an accuracy of 92\%.},
	booktitle = {2021 4th {International} {Conference} on {Electrical}, {Computer} and {Communication} {Technologies}, {ICECCT} 2021},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Niranjan, K. and Shankar Kumar, S. and Vedanth, S.},
	year = {2021},
	keywords = {Artigo, Conferência, Convolution Neural Network, Deep Learning, Ensemble, Multi-Model, Sound Classification, Spectrograms},
}

@inproceedings{Bubashait2021,
	title = {Urban {Sound} {Classification} {Using} {DNN}, {CNN} {LSTM} a {Comparative} {Approach}},
	isbn = {978-1-66544-032-5},
	doi = {10.1109/3ICT53449.2021.9581339},
	abstract = {Like air pollution, sound pollution has grown to be a major concern for city residents, designers, and developers. Detecting and recognizing sound types and sources in cities and suburban areas or any environment have become a necessity for the quality of life as well as security. In recent years, researchers have explored many models using Convolutional Neural Network (CNN), Long Short Term Memory (LSTM) Neural Network, and different combinations of these techniques, which produced promising results when combined with spectrogram images, or its different variations, to classify urban sounds. This research uses the DNN model performance as a baseline to compare the CNN and LSTM models' performance for classifying urban sound using Mel scale cepstral analysis (MEL) spectrum images using an open-source library called Librosa for sound processing. Models' performance was evaluated using the UrbanSound8k dataset. The CNN model underperformed with an accuracy rate of 87.15\% and f1 score of 85.63\% compared to both the DNN base model and the LSTM model. In contrast, comparing the LSTM model with CNN, LSTM shows better accuracy performance on test data with 90.15\%, and f1 score of 90.15\%.},
	booktitle = {2021 {International} {Conference} on {Innovation} and {Intelligence} for {Informatics}, {Computing}, and {Technologies}, {3ICT} 2021},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Bubashait, Mohamed and Hewahi, Nabil},
	month = sep,
	year = {2021},
	keywords = {Artigo, CNN, CRNN, Conferência, DNN, LSTM, Librosa, Sound Classification, Spectrograms},
	pages = {46--50},
}

@article{Castillejo2014,
	title = {Modelling users, context and devices for adaptive user interface systems},
	volume = {10},
	issn = {1742-7371},
	url = {https://www.emerald.com/insight/content/doi/10.1108/IJPCC-09-2013-0028/full/html},
	doi = {10.1108/IJPCC-09-2013-0028},
	abstract = {Purpose: The purpose of this paper is to review the state-of-the-art in adaptive user interface systems by studying their historical development over the past 20 years. Moreover, this paper contributes with a specific model combining three main entities (users, context and devices) that have been demonstrated to be always represented in these environments. Novel concepts that should be taken into account in these systems are also presented. Design/methodology/approach: The authors first provide a review and a comparison of current user interface adaptive systems. Next, the authors detail the most significant models and the set of techniques used to, finally, propose a novel model based on the studied literature. Findings: Literature solutions for adaptive user interface systems tend to be very domain dependant. This situation restricts the possibility of sharing and exporting the information between such systems. Furthermore, the studied approaches barely highlight the dynamism of these models. Originality/value: The paper is a review of adaptive user interface systems and models. Although there are several reviews in this area, there is a lack of research for modelling users, context and devices simultaneously in this domain. The paper also presents several significant concepts that should be taken into account to bring an adaptive and dynamic perspective to the studied models. © Emerald Group Publishing Limited.},
	number = {1},
	journal = {International Journal of Pervasive Computing and Communications},
	author = {Castillejo, Eduardo and Almeida, Aitor and López-de-Ipiña, Diego},
	editor = {Liming Luke Chen, Dr Rene Mayrhofer, Prof},
	month = apr,
	year = {2014},
	note = {Publisher: Emerald Group Publishing Ltd.},
	keywords = {Adaptive user interfaces, Context modelling, Context-aware computing, Context-aware systems, Mobile applications, Mobile computing, PEL214, Pervasive computing, User adaptability, User modelling, projeto\_IHC},
	pages = {69--91},
}

@article{Brdnik2022,
	title = {Intelligent {User} {Interfaces} and {Their} {Evaluation}: {A} {Systematic} {Mapping} {Study}},
	volume = {22},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/22/15/5830},
	doi = {10.3390/s22155830},
	abstract = {{\textless}p{\textgreater}Intelligent user interfaces (IUI) are driven by the goal of improvement in human–computer interaction (HCI), mainly improving user interfaces’ user experience (UX) or usability with the help of artificial intelligence. The main goal of this study is to find, assess, and synthesize existing state-of-the-art work in the field of IUI with an additional focus on the evaluation of IUI. This study analyzed 211 studies published in the field between 2012 and 2022. Studies are most frequently tied to HCI and SE domains. Definitions of IUI were observed, showing that adaptation, representation, and intelligence are key characteristics associated with IUIs, whereas adaptation, reasoning, and representation are the most commonly used verbs in their description. Evaluation of IUI is mainly conducted with experiments and questionnaires, though usability and UX are not considered together in evaluations. Most evaluations (81\% of studies) reported partial or complete improvement in usability or UX. A shortage of evaluation tools, methods, and metrics, tailored for IUI, is noticed. Most often, empirical data collection methods and data sources in IUI evaluation studies are experiment, prototype development, and questionnaire.{\textless}/p{\textgreater}},
	number = {15},
	journal = {Sensors},
	author = {Brdnik, Saša and Heričko, Tjaša and Šumak, Boštjan},
	month = aug,
	year = {2022},
	note = {Publisher: MDPI},
	keywords = {IUI, PEL214, evaluation, intelligent user interfaces, projeto\_IHC, usability, user experience},
	pages = {5830},
}

@article{sierpowska_white-matter_2019,
	title = {White-matter pathways and semantic processing: intrasurgical and lesion-symptom mapping evidence},
	volume = {22},
	issn = {2213-1582},
	url = {https://www.sciencedirect.com/science/article/pii/S2213158219300543},
	doi = {https://doi.org/10.1016/j.nicl.2019.101704},
	abstract = {In the present study, we aimed to test the association between the correct function of the left ventral white matter pathways and semantic processing (dual stream models for language processing, Hickok \& Poeppel, 2004), using a new set of language tasks during intraoperative electrical stimulation at white matter level. Additionally, we evaluated brain regions needed for correct performance on the different semantic tasks using lesion-symptom analyses (voxel lesion-symptom mapping and track-wise lesion analysis) in a sample of 62 candidates for the awake brain surgery. We found that electrical stimulation in the vicinity of the inferior longitudinal and inferior fronto-occipital fasciculi disturbed performance on semantic processing tasks. Individuals presented with significantly more semantic paraphasias during brain tumor resection than during the electrical stimulation at the cortex level. Track-wise analyses confirmed the role of these left ventral pathways in semantic processing: a significant relationship was observed between the probability of inferior fronto-occipital fasciculus disconnection/damage and the semantic matching tasks, as well as the number of semantic paraphasias in naming. Importantly, the same analyses for the total score of the Boston Naming Test confirmed significant relationships between this test score and the integrity of the inferior fronto-occipital, inferior longitudinal and uncinate fasciculi. This was further supported by the results of VLSM analyses showing a significant relationship between BNT and the presence of lesion within left middle and inferior temporal gyri. The present findings provide new intraoperative evidence for the role of the white-matter ventral pathways in semantic processing, while at the same time emphasizing the need to include a broader assessment of semantic-conceptual aspects during the awake neurosurgical intervention. This approach will ensure better preservation of functional tissue in the tumoral vicinity and therefore substantially diminish post-surgical language impairments.},
	journal = {NeuroImage: Clinical},
	author = {Sierpowska, Joanna and Gabarrós, Andreu and Fernández-Coello, Alejandro and Camins, Àngels and Castañer, Sara and Juncadella, Montserrat and François, Clément and Rodríguez-Fornells, Antoni},
	year = {2019},
	keywords = {Diffusion tensor imaging (DTI), Dual stream models for language processing, ESM = electrical stimulation mapping, Environment\_Sound\_Recognition, Science\_Direct, Semantic processing, Voxel lesion-symptom mapping (VLSM)},
	pages = {101704},
}

@misc{WIKIPEDIA_UI2022,
	title = {User interface},
	url = {https://en.wikipedia.org/wiki/User_interface},
	urldate = {2022-10-05},
	author = {{Wikipedia}},
	month = oct,
	year = {2022},
	note = {Medium: https://en.wikipedia.org/wiki/User\_interface},
	keywords = {PEL214},
}

@book{Nielsel1994,
	address = {Oxford, England},
	title = {Usability {Engineering}},
	isbn = {0-12-518406-9},
	language = {en},
	publisher = {Morgan Kaufmann},
	author = {Nielsen, Jakob},
	month = nov,
	year = {1994},
	note = {Series Title: Interactive Technologies},
	keywords = {PEL214, projeto\_IHC},
}

@misc{WIKIPEDIA_UM2022,
	title = {Universal design},
	url = {https://en.wikipedia.org/wiki/Universal_design},
	urldate = {2022-10-05},
	author = {{Wikipedia}},
	month = oct,
	year = {2022},
	note = {Medium: https://en.wikipedia.org/wiki/Universal\_design},
	keywords = {PEL214},
}

@book{Rossing2013,
	address = {London, England},
	edition = {3},
	title = {The science of sound: {Pearson} new international edition},
	isbn = {978-1-292-03957-2},
	language = {en},
	publisher = {Pearson Education},
	author = {Rossing, Thomas D and Moore, Richard F and Wheeler, Paul A},
	month = nov,
	year = {2013},
}

@article{singh_robustness_2022,
	title = {Robustness of musical features on deep learning models for music genre classification},
	volume = {199},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422003244},
	doi = {https://doi.org/10.1016/j.eswa.2022.116879},
	abstract = {Music information retrieval (MIR) has witnessed rapid advances in various tasks like musical similarity, music genre classification (MGC), etc. MGC and audio tagging are approached using various features through traditional machine learning and deep learning (DL) based techniques by many researchers. DL-based models require a large amount of data to generalize well on new data samples. Unfortunately, the lack of sizeable open music datasets makes the analyses of the robustness of musical features on DL models even more necessary. So, this paper assesses and compares the robustness of some commonly used musical and non-musical features on DL models for the MGC task by evaluating the performance of selected models on multiple employed features extracted from various datasets accounting for billions of segmented data samples. In our evaluation, Mel-Scale based features and Swaragram showed high robustness across the datasets over various DL models for the MGC task.},
	journal = {Expert Systems with Applications},
	author = {Singh, Yeshwant and Biswas, Anupam},
	year = {2022},
	keywords = {Deep learning, Environment\_Sound\_Classification, Music genre classification, Music information retrieval, Musical features, Science\_Direct},
	pages = {116879},
}

@article{zhang_robust_2021,
	title = {Robust acoustic event recognition using {AVMD}-{PWVD} time-frequency image},
	volume = {178},
	issn = {0003-682X},
	url = {https://www.sciencedirect.com/science/article/pii/S0003682X21000633},
	doi = {https://doi.org/10.1016/j.apacoust.2021.107970},
	abstract = {Environmental sound feature extraction and classification are important signal analysis tools in many applications, such as audio surveillance, multimedia retrieval, and auditory source identification. However, the non-stationarity and discontinuity of environmental signals make quantification and classification a formidable challenge. Hence, researchers proposed to use the time-frequency image representation to quantify these non-stationarity, resulting in higher classification accuracy. In this paper, a time-frequency representation method is proposed to represent environmental sound signals. Our approach consists of three stages: Firstly, we propose an adaptive variational modal decomposition (AVMD) based on central angular frequency difference to decompose environmental sounds into a series of modes. Secondly, we use the pseudo Wigner-Vile distribution (PWVD) to accurately obtain the instantaneous frequency characteristics of mode signals. Thirdly, time-frequency images of sound signals are obtained by combining the mode signals with PWVD. Finally, we put the time-frequency image into a convolutional neural network (CNN) for classification. The method is tested on the Real World Computing Partnership (RWCP) Sound Scene Database of 50 classes in mismatched conditions. Results show that our method is robust to noise and achieves the best average recognition accuracy compared with several state-of-art methods under clean and various noisy conditions.},
	journal = {Applied Acoustics},
	author = {Zhang, Yanhua and Zhang, Ke and Wang, Jingyu and Su, Yu},
	year = {2021},
	keywords = {Acoustic event recognition, Convolutional neural network, Environment\_Sound\_Classification, Pseudo Wigner-Vile distribution, Pseudo-color, Science\_Direct, Time-frequency image, Variational modal decomposition},
	pages = {107970},
}

@book{Shneiderman2022,
	address = {London, England},
	title = {Human-{Centered} {AI}},
	isbn = {0-19-284529-2},
	abstract = {Researchers, developers, business leaders, policy makers, and 
others are expanding the technology-centered scope of artificial 
intelligence (AI) to include human-centered AI (HCAI) ways of 
thinking. This expansion from an algorithm-focused view to 
embrace a human-centered perspective can shape the future of 
technology so as to better serve human needs. Educators, 
designers, software engineers, product managers, evaluators, and 
government agency staffers can build on AI-driven technologies 
to design products and services that make life better for people 
and enable people to care for each other. Humans have always 
been tool builders, and now they are supertool builders, whose 
inventions can improve our health, family life, education, 
business, the environment, and much more. The remarkable 
progress in algorithms for machine and deep learning have opened 
the doors to new opportunities, and some dark possibilities. 
However, a bright future awaits AI researchers, developers, 
business leaders, policy makers, and others who build on their 
working methods by including HCAI strategies of design and 
testing. This enlarged vision can shape the future of technology 
so as to better serve human needs. As many technology companies 
and thought leaders have said, the goal is not to replace 
people, but to empower them by making design choices that give 
humans control over technology.},
	publisher = {Oxford University Press},
	author = {Shneiderman, Ben},
	month = jan,
	year = {2022},
	keywords = {PEL214, projeto\_IHC},
}

@article{peng_environmental_2021,
	title = {Environmental {Sound} {Recognition} {Based} on {Attention} {Sinusoidal} {Representation} {Network} {\textbar} 基于正弦注意力表征网络的环境声音识别},
	volume = {39},
	doi = {10.3969/j.issn.0255-8297.2021.04.011},
	abstract = {In this paper, we propose an attention sinusoidal representation network (ASIREN). Firstly, Mel -frequency cepstral coefficient (MFCC) as an audio recognition feature is extracted from a dataset. Then, feature extraction is performed on each frame of the MFCC by using a neural network named gated recurrent unit (GRU). And audio score is calculated for each frame by using sine function and the audio is re-weighted according to the audio score of each frame. Finally, the categories of environmental sound are discriminated by using the full connection layer in combination with the Softmax classifier. In the experiments of this paper, we validated the designed model in an open-source dataset Urban Sound 8K and compared the performance of the designed model with that of other models. Experimental results show that the A-SIREN works best on the Urban Sound 8K dataset with recognition rate as high as 93.5\%.},
	number = {4},
	journal = {Yingyong Kexue Xuebao/Journal of Applied Sciences},
	author = {Peng, N. and Chen, A. and Zhou, G. and Chen, W. and Liu, J.},
	year = {2021},
	keywords = {Environment\_Sound\_Recognition, SCOPUS},
	pages = {641--649},
}

@inproceedings{zhang_environment_2020,
	title = {Environment {Sound} {Classification} {System} {Based} on {Hybrid} {Feature} and {Convolutional} {Neural} {Network}},
	author = {Zhang, Kecheng and Su, Yu and Wang, Jingyu and Wang, Sanyu and Zhang, Yanhua},
	year = {2020},
	keywords = {Environment\_Sound\_Classification, Semantic\_Scholar},
}

@inproceedings{sharma_e_2021,
	title = {E {Environment} {Sound} {Classification} using {Multiple} {Feature} {Channels} and {Attention} based {Deep} {Convolutional} {Neural} {Network}},
	author = {Sharma, Jivitesh and Granmo, Ole-Christoffer and Goodwin, Morten},
	year = {2021},
	keywords = {Environment\_Sound\_Classification, Semantic\_Scholar},
}

@inproceedings{tang_deep_2019,
	title = {Deep {CNN} {Framework} for {Environmental} {Sound} {Classification} using {Weighting} {Filters}},
	isbn = {978-1-72811-698-3},
	doi = {10.1109/ICMA.2019.8816567},
	abstract = {Deep convolutional neural networks have been used to classify environmental sound recently. The classification system with high performance often requires a large well-labled dataset. The cost of tagging audio segments correctly and completely is quite high thus the deep learning models need to have high generalization ability if a weakly-tagged dataset is used. An algorithm named Weighting Filters algorithm(WF) which can be considered as an improved algorithm based on Dropout is proposed in this paper to enhance the generalization ability of models. To implement the Weighting Filters algorithm, an extra layer trained by backpropagation algorithm is introduced to produce a series of weighted filters. The simulation results show that the Weighting Filters algorithm is an effective way to improve the generalization ability of the model. Further more, a deep convolutional neural network using weighting filters algorithm is proposed for the applications of environmental sound classification. The main contributions of this paper are as follows: First, we proposed an effective algorithm WF based on Dropout, and secondly, we proposed a CNN-based framework using WF(CNN-WF) for environmental sound classification. The results obtained on ESC-50 demonstrate that the CNN-based framework we proposed has considerable performance for environmental sound classification.},
	booktitle = {Proceedings of 2019 {IEEE} {International} {Conference} on {Mechatronics} and {Automation}, {ICMA} 2019},
	author = {Tang, B. and Li, Y. and Li, X. and Xu, L. and Yan, Y. and Yang, Q.},
	year = {2019},
	keywords = {Environment\_Sound\_Classification, SCOPUS},
	pages = {2297--2302},
}

@article{vakamullu_convolutional_2022,
	title = {Convolutional {Neural} {Network} {Based} {Heart} {Sounds} {Recognition} on {Edge} {Computing} {Platform}},
	journal = {2022 IEEE International Instrumentation and Measurement Technology Conference (I2MTC)},
	author = {Vakamullu, Venkatesh and Trivedy, Sudipto and Mishra, Madhusudhan and Mukherjee, Anirban},
	year = {2022},
	keywords = {Environment\_Sound\_Recognition, Semantic\_Scholar},
	pages = {1--6},
}

@misc{WIKIPEDIA_CM2022,
	title = {Context model},
	url = {https://en.wikipedia.org/wiki/Context_model},
	urldate = {2022-10-23},
	author = {{Wikipedia}},
	month = oct,
	year = {2022},
	keywords = {PEL214},
}

@article{okaba_automated_2021,
	title = {An automated location detection method in multi-storey buildings using environmental sound classification based on a new center symmetric nonlinear pattern: {CS}-{LBlock}-{Pat}},
	volume = {125},
	issn = {0926-5805},
	url = {https://www.sciencedirect.com/science/article/pii/S0926580521000960},
	doi = {https://doi.org/10.1016/j.autcon.2021.103645},
	abstract = {Classical navigations applications only give latitude and longitude information, thus, location detection has been processed in two-dimensional(2D) space by using latitude and longitude. There are many multi-storey buildings in cities, and these buildings cause location detection problem in 3D space. This research presents a solution for solving this problem. An environmental sound classification(ESC) dataset was collected from a multi-storey hospital, and an automated ESC model is presented. As a feature generation function, a new center symmetric nonlinear pattern is presented using a substitution box(S-Box) of LBlock lightweight block cipher., therefore, this model is named CS-LBlock-Pat. The presented model is applied to the dataset collected from a multi-storey hospital. This hospital has ten floors. Therefore, ten classes of classification results are given. This model yielded 95.38\% accuracy rate using SVM. The results obtained from the classification phase obviously demonstrated that the 3D location detection problem could be solved by using ESC.},
	journal = {Automation in Construction},
	author = {Okaba, Mark and Tuncer, Turker},
	year = {2021},
	keywords = {3D location detection in multi-storey buildings, Acoustical floor identification, Center symmetric LBlock pattern, Environment\_Sound\_Classification, Environmental sound classification, Iterative neighborhood component analysis, Science\_Direct, Statistical feature extraction},
	pages = {103645},
}

@book{Mayhew1999,
	address = {Oxford, England},
	title = {The {Usability} {Engineering} lifecycle: {A} practitioner's handbook for user interface design},
	isbn = {978-1-55860-561-9},
	abstract = {This text is about achieving usability in product user interface
design through a process called Usability Engineering. The
techniques presented include not only UI requirements analysis,
but also organizational and managerial strategies.},
	language = {en},
	publisher = {Morgan Kaufmann},
	author = {Mayhew, Deborah J},
	month = mar,
	year = {1999},
	keywords = {PEL214, projeto\_IHC},
}

@article{liu_research_2022,
	title = {Research on {Acoustic} {Events} {Recognition} {Method} {With} {Dimensionality} {Reduction} {Combining} {Attention} and {Mutual} {Information}},
	volume = {22},
	doi = {10.1109/JSEN.2022.3155706},
	abstract = {The environment sound classification(ESC) is of great significance to the monitoring and control of urban noise. Aiming at the curse of dimensionality phenomenon in ESC, a feature dimensionality reduction architecture combining attention and mutual information is proposed. In order to match the two-dimensional MFCC (Mel Frequency Cepstral Coefficients) feature matrix, the proposed method separates and reconstructs the feature frames of different samples, and achieves the effect of dimensionality reduction by making decisions on the information entropy between the feature frames and labels. In addition, the method combines LSTM (Long Short-Term Memory) model with attention mechanism to ensure the recognition accuracy of the model after dimensionality reduction. Ten urban acoustic events from UrbanSound8k (US8K) dataset are selected to verify the performances of the proposed method by simulation experiments, which are also compared with the existing classification methods. The simulation results show that by combining the attention mechanism and mutual information, the recognition accuracy of the proposed method on the UrbanSound8k dataset is 95.16\%, and the parameter scale is the smallest, only 0.92M. Moreover, the model parameter scale is adjustable by dynamic frame retention mechanism to balance the recognition accuracy and speed. This method not only ensures a high classification accuracy, but also can reduce computing power consumption and storage space of monitoring equipment, which shows a better practical performance for urban acoustic events recognition.},
	number = {9},
	journal = {IEEE Sensors Journal},
	author = {Liu, H. and Zhou, J. and Xi, G. and Peng, B. and Zhang, S. and Xiao, Q.},
	year = {2022},
	keywords = {Environment\_Sound\_Classification, SCOPUS},
	pages = {8622--8632},
}

@inproceedings{lin_environmental_2020,
	title = {Environmental {Sound} classification},
	author = {Lin, Tong},
	year = {2020},
	keywords = {Environment\_Sound\_Recognition, Semantic\_Scholar},
}

@article{li_robustness_2022,
	title = {Robustness of {Neural} {Architectures} for {Audio} {Event} {Detection}},
	volume = {abs/2205.03268},
	journal = {ArXiv},
	author = {Li, Juncheng Billy and Qu, Shuhui and Metze, Florian},
	year = {2022},
	keywords = {Environment\_Sound\_Classification, Semantic\_Scholar},
}

@article{li_evaluation_2022,
	title = {Evaluation of hearing and speech rehabilitation after cochlear implantation in children with {Waardenburg} syndrome.},
	volume = {36 5},
	journal = {Lin chuang er bi yan hou tou jing wai ke za zhi = Journal of clinical otorhinolaryngology, head, and neck surgery},
	author = {Li, Guo-Yang and Liu, Li and Yang, Ting and Lang, Chunmei and Ma, Xiuli and Zhao, Liping and Tang, Xianchao and Wang, Xiang and Zhang, Tiesong and Ma, Jing},
	year = {2022},
	keywords = {Environment\_Sound\_Recognition, Semantic\_Scholar},
	pages = {347--352},
}

@article{hidayat_convolutional_2021,
	title = {Convolutional {Neural} {Networks} for {Scops} {Owl} {Sound} {Classification}},
	volume = {179},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050920324492},
	doi = {https://doi.org/10.1016/j.procs.2020.12.010},
	abstract = {Adopting a deep learning model into bird sound classification tasks becomes a common practice in order to construct a robust automated bird sound detection system. In this paper, we employ a four-layer Convolutional Neural Network (CNN) formulated to classify different species of Indonesia scops owls based on their vocal sounds. Two widely used representations of an acoustic signal: log-scaled mel-spectrogram and Mel Frequency Cepstral Coefficient (MFCC) are extracted from each sound file and fed into the network separately to compare the model performance with different inputs. A more complex CNN that can simultaneously process the two extracted acoustic representations is proposed to provide a direct comparison with the baseline model. The dual-input network is the well-performing model in our experiment that achieves 97.55\% Mean Average Precision (MAP). Meanwhile, the baseline model achieves a MAP score of 94.36\% for the mel-spectrogram input and 96.08\% for the MFCC input.},
	journal = {Procedia Computer Science},
	author = {Hidayat, Alam Ahmad and Cenggoro, Tjeng Wawan and Pardamean, Bens},
	year = {2021},
	keywords = {Environment\_Sound\_Classification, Science\_Direct, acoustic features, bird sound classification, convolutional neural network, mean average precision, scops owl},
	pages = {81--87},
}

@inproceedings{gloaguen_study_2019,
	title = {{STUDY} {OF} {THE} {NON}-{NEGATIVE} {MATRIX} {FACTORIZATION} {BE}- {HAVIOR} {TO} {ESTIMATE} {THE} {URBAN} {TRAFFIC} {SOUND} {LEVELS}},
	author = {Gloaguen, Jean-Rémy and Lagrange, Mathieu and Can, Arnaud and Petiot, Jean-François},
	year = {2019},
	keywords = {Environment\_Sound\_Recognition, Semantic\_Scholar},
}

@article{fan_application_2020,
	title = {The application design of hearing aid parameters auto adaptive system for hearing impaired children based on android terminal},
	journal = {2020 International Conference on Information Science and Education (ICISE-IE)},
	author = {Fan, Xiaoqian and Chen, Wenzhi and Fan, Quanfang and Sun, Tianyi},
	year = {2020},
	keywords = {Environment\_Sound\_Classification, Semantic\_Scholar},
	pages = {241--245},
}

@article{dong_environment_2020,
	title = {Environment {Sound} {Event} {Classification} {With} a {Two}-{Stream} {Convolutional} {Neural} {Network}},
	volume = {8},
	journal = {IEEE Access},
	author = {Dong, Xifeng and Yin, Bo and Cong, Yanping and Du, Zehua and Huang, Xianqing},
	year = {2020},
	keywords = {Environment\_Sound\_Classification, Semantic\_Scholar},
	pages = {125714--125721},
}

@inproceedings{Du2022,
	address = {Dearborn, USA},
	title = {A {Study} on {Speech} {Intelligibility} {Performance} of {Automotive} {Voice} {Microphones}},
	volume = {2022-June},
	url = {http://www.aes.org/e-lib/browse.cfm?elib=21803},
	urldate = {2023-05-27},
	booktitle = {Procedings [...]},
	publisher = {Audio Engineering Society},
	author = {Du, Yu and Varga, Bálaz and Dobos, Viktor},
	year = {2022},
	note = {tex.venue: Dearborn, USA
tex.eventyear: 2022},
	pages = {p. (paper 2)},
}

@article{cesur_data_2020,
	title = {Data logging variables and speech perception in prelingually deafened pediatric cochlear implant users},
	volume = {133},
	issn = {0165-5876},
	url = {https://www.sciencedirect.com/science/article/pii/S0165587620301464},
	doi = {https://doi.org/10.1016/j.ijporl.2020.110003},
	abstract = {Objectives
To investigate the relationship among objectively gathered data logging measurements, patient-related variables, and speech recognition performance of pediatric CI users.
Methods and materials
Thirty-two prelingually implanted children who have the ability to perform word discrimination test were included in this study. To reveal the relationship between speech perception abilities and auditory exposure, seven data logging variables were analyzed: “on-air,” “off-air,” “coil-off,” “speech,” “speech in noise,” “music” and “noise. In addition, implantation age (months) and CI usage duration (months) were taken into account. Finally, it was investigated the differences between unilateral, sequential bilateral, and simultaneous bilateral CI users in terms of all study variables.
Results
The average on-air time ranged between 10.52 and 12.30 in the groups. In the case of sequential implantation, smaller on-air and higher coil off values were observed with the second CI. In the case of simultaneous bilateral implantation, data logging measurements were almost the same in both implants. WRS was significantly correlated (p {\textless} 0.05) with on-air time (r = 0.62), coil-off count (r = −0.48), chronological age (r = 0.48), and CI duration (r = 0.44). Multiple linear regression model was fit to predict the WRS, with on-air time, CI duration, and chronological age as predictors.
Conclusions
The critical importance of early intervention and long-term use of CI is well-established in the literature and is also corroborated by our findings. However, the key findings of the present study are that consistent CI use and the quality of daily listening environment also exerted a major and positive effect on the speech recognition performance of pediatric CI users. Therefore, during the monitoring of pediatric CI recipients, it is important to know the device usage data in order to detect problems in the early stages after CI.},
	journal = {International Journal of Pediatric Otorhinolaryngology},
	author = {Cesur, Sıdıka and Yüksel, Mustafa and Çiprut, Ayça},
	year = {2020},
	keywords = {Cochlear implant, Data logging, Environment\_Sound\_Classification, Listening environment, Science\_Direct, Speech recognition},
	pages = {110003},
}

@article{boztepe_approach_2022,
	title = {An {Approach} for {Audio}-{Visual} {Content} {Understanding} of {Video} using {Multimodal} {Deep} {Learning} {Methodology}},
	journal = {Sakarya University Journal of Computer and Information Sciences},
	author = {Boztepe, Emre Beray and Karakaya, Bedirhan and Karasulu, Bahadir and Ünlü, İsmet},
	year = {2022},
	keywords = {Environment\_Sound\_Recognition, Semantic\_Scholar},
}

@article{adeodu_adaptive_2020,
	title = {An adaptive {Industrial} {Internet} of things ({IIOts}) based technology for prediction and control of cavitation in centrifugal pumps},
	volume = {91},
	issn = {2212-8271},
	url = {https://www.sciencedirect.com/science/article/pii/S2212827120309240},
	doi = {https://doi.org/10.1016/j.procir.2020.03.125},
	abstract = {Cavitation is the phenomenon whereby the air bubbles implode on the impeller due to insufficient suction pressure at the inlet section thereby causing noise, vibration and in a long-term, corrosion of the impeller blades. However, the current devices and technologies used to address cavitation are inefficient. The use of intelligence gathered from telemetry data produced via cloud computing and IOT is becoming prevalent in the prediction and control of cavitation. The objective of this research is to simulate cavitation in industrial pumps using the IOT technology. Cavitation was simulated in both good and faulty centrifugal pumps using the Fast Fourier Transform Algorithm (FFT) integrated to an Arduino microcontroller. Subsequently, configuration of the sensors required to capture the data, ingestion into the cloud and numerical analysis of the data were executed using the Artificial Neural Network (ANN) as the machine learning algorithm. The results showed graphs as well as the tables of non-cavitation and cavitation patterns when the ball valve was fully opened (90º) respectively, at the suction of the centrifugal pump. For the non-cavitation patterns, the voltages lie from 0.5-0.75v and for the cavitation patterns, the voltages lie from 0.76-0.95v. Using this technique, the detection, analysis and accurate prediction of cavitation were established.},
	journal = {Procedia CIRP},
	author = {Adeodu, Adefemi and Daniyan, Ilesanmi and Omitola, Olusegun and Ejimuda, Chinoyelum and Agbor, Esoso and Akinola, Oluwole},
	year = {2020},
	keywords = {Cavitation, Environment\_Sound\_Recognition, Science\_Direct, corossion, impeller, internet of things, micro controller},
	pages = {927--934},
}

@article{ahmad_environmental_2020,
	title = {Environmental sound classification using optimum allocation sampling based empirical mode decomposition},
	volume = {537},
	issn = {0378-4371},
	url = {https://www.sciencedirect.com/science/article/pii/S0378437119314955},
	doi = {https://doi.org/10.1016/j.physa.2019.122613},
	abstract = {Automatic environmental sound classification (ESC) is prominent in various fields like robotics, security, and crime investigation. In this paper, optimum allocation sampling (OAS)-based empirical mode method (EMD) is proposed for automatic ESC. The OAS provides the reduced homogeneous length sequence of each long length sound signal, which is further decomposed into band-limited intrinsic mode functions (IMFs) using EMD. The features namely approximate entropy (AE), permutation entropy (PE), log energy entropy (LE), interquartile range (IQR), and zero cross rate (ZCR) are extracted from the IMFs. The OAS-EMD based features used as input to multi-class least squares support vector machine (MC-LS-SVM) and extreme learning machine (ELM) classifiers for evaluation the performance of proposed method. Experimental results show an accuracy of 87.25\% and 77.61\% with MC-LS-SVM and ELM classifiers, respectively.},
	journal = {Physica A: Statistical Mechanics and its Applications},
	author = {Ahmad, Saad and Agrawal, Shubham and Joshi, Samta and Taran, Sachin and Bajaj, Varun and Demir, Fatih and Sengur, Abdulkadir},
	year = {2020},
	keywords = {Empirical mode decomposition, Environment\_Sound\_Classification, Environmental sound classification, Extreme learning machine, Multi-class least squares support vector machine, Optimum allocation sampling, Science\_Direct},
	pages = {122613},
}

@phdthesis{Wouter2021,
	address = {Brussels},
	title = {{DNN} for {Urban} {Sound} {Recognition} on an {FPGA}},
	url = {https://researchportal.vub.be/nl/studentTheses/embedding-deep-neural-networks-for-urban-sound-recognition-on-an-},
	urldate = {2022-09-28},
	school = {Vrije Universiteit Brussel},
	author = {Wouters, Nick},
	year = {2021},
	keywords = {Dissertação},
}

@article{Webb2001,
	title = {Machine {Learning} for {User} {Modeling}},
	volume = {11},
	issn = {1573-1391},
	url = {https://doi.org/10.1023/A:1011117102175},
	doi = {10.1023/A:1011117102175},
	abstract = {At first blush, user modeling appears to be a prime candidate for straightforward application of standard machine learning techniques. Observations of the user's behavior can provide training examples that a machine learning system can use to form a model designed to predict future actions. However, user modeling poses a number of challenges for machine learning that have hindered its application in user modeling, including: the need for large data sets; the need for labeled data; concept drift; and computational complexity. This paper examines each of these issues and reviews approaches to resolving them.},
	number = {1},
	journal = {User Modeling and User-Adapted Interaction},
	author = {Webb, Geoffrey I and Pazzani, Michael J and Billsus, Daniel},
	year = {2001},
	keywords = {PEL214},
	pages = {19--29},
}

@incollection{Rybak2007,
	address = {Boston, MA},
	title = {Electromagnetic {Fields}},
	isbn = {978-1-4020-7713-5},
	language = {en},
	booktitle = {Automotive {Electromagnetic} {Compatibility} ({EMC})},
	publisher = {Springer US},
	author = {Rybak, Terence and Steffka, Mark},
	editor = {Rybak, Terence and Steffka, Mark},
	year = {2007},
	keywords = {Microphone},
	pages = {91--114},
}

@book{Rumreich2005,
	address = {New York, NY},
	edition = {2},
	title = {Car {Stereo} {Cookbook}},
	isbn = {978-0-07-144847-5},
	language = {en},
	publisher = {McGraw-Hill Professional},
	author = {Rumreich, Mark},
	month = apr,
	year = {2005},
}

@inproceedings{Rogers2000,
	title = {Adaptive user interfaces for automotive environments},
	isbn = {0-7803-6363-9},
	url = {http://ieeexplore.ieee.org/document/898424/},
	doi = {10.1109/IVS.2000.898424},
	abstract = {Our research group is investigating the use of adaptive user interfaces for in-car information access. These interfaces attempt to efficiently provide content the driver needs and wants, and gather feedback on these preferences through the driver's interaction with the system. In this way, the performance of the system improves as it unobtrusively builds a more accurate model of the user. The three systems presented here are the Adaptive Route Advisor for navigation, the Adaptive News Reader for news stories, and the Adaptive Place Advisor for restaurant selection. All of these systems provide useful information to a driver, and we argue they do not negatively impact safety because they are replacing other, less effective, information sources. We intend to test this hypothesis in future studies.},
	booktitle = {Proceedings of the {IEEE} {Intelligent} {Vehicles} {Symposium} 2000 ({Cat}. {No}.{00TH8511})},
	publisher = {IEEE},
	author = {Rogers, Seth and Fiechter, Claude-Nicolas and Thompson, Cynthia},
	year = {2000},
	keywords = {PEL214, projeto\_IHC},
	pages = {662--667},
}

@book{Rayburn2004,
	address = {Oxford, England},
	edition = {2},
	title = {Eargle's the microphone book},
	isbn = {978-0-240-51961-6},
	language = {en},
	publisher = {Focal Press},
	author = {Rayburn, Ray A and Eargle, John},
	month = apr,
	year = {2004},
}

@book{Preece2015,
	address = {Nashville, TN},
	title = {Interaction {Design}: {Beyond} {Human}-{Computer} {Interaction}},
	isbn = {978-1-119-02075-2},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Preece, Jenny and Sharp, Helen and Rogers, Yvonne},
	month = feb,
	year = {2015},
	keywords = {PEL214},
}

@book{Newcomb2008,
	address = {Chichester, England},
	edition = {1},
	title = {Car {Audio} {For} {Dummies}},
	isbn = {978-0-470-15158-7},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Newcomb, Doug},
	month = jan,
	year = {2008},
}

@book{Moore2013,
	address = {Leiden, Netherlands},
	edition = {1},
	title = {An introduction to the psychology of hearing},
	isbn = {90-04-25242-8},
	language = {en},
	publisher = {Brill},
	author = {Moore, Brian},
	month = apr,
	year = {2013},
}

@article{Miraz2021,
	title = {Adaptive user interfaces and universal usability through plasticity of user interface design},
	volume = {40},
	issn = {15740137},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1574013721000034},
	doi = {10.1016/j.cosrev.2021.100363},
	abstract = {A review of research on universal usability, plasticity of user interface design and facilitation of interface development with universal usability is presented. The survey was based on 165 research papers spanning over fifty-five years. The foundations of adaptive or intelligent user interfaces (AUI or IUI) are presented, three core domains being focused upon: Artificial Intelligence (AI), User Modelling (UM) and Human-Computer Interaction (HCI). For comparison of the various AUIs, a proposed taxonomy is given. One conclusion is that an efficient training vector for fast optimal convergence of the machine-learning algorithm is a necessity, but key to this is the bounding of the dataset, the goal being to achieve an accurate user preference model, which has to be built from a limited number of datasets obtained from the human interaction. More research also needs to be conducted to ascertain the usefulness and effectiveness of IUIs compared against AUIs. With the global mobility of users, interface design must take account of the abilities and cultures of users, derived from actual user behaviour and not on their feedback. A key question is whether the interface should be adaptive under system control or be made adaptable under user control. A need is identified for an "afferential component"that stores a priori information about the end user, an "inferential component"that determines to what extent the user interface actually needs to be adapted, and the "efferential component"that actually determines how the adaptivity is applied seamlessly to the system. Application to e-learning is a priority: the use of machine intelligence to achieve appropriate learnability, ideally enhanced by "Playful interaction", was found to be desirable. Universal application of adaptation lies in the future, but AUI properties cannot be ascertained while disregarding the other parameters of the system in which it will be used. A more complete understanding of the human mental model is necessary, requiring a highly multidisciplinary approach and cooperation between diverse researchers. Finally, a performance evaluation of plasticity of user interface was conducted: it is concluded that the use of dynamic techniques can enhance the user experience to a much greater extent than more basic approaches, although optimisation of usability parameter trade-offs needs further attention. It is noted that most of the work reviewed originated from a limited range of cultural perspectives. To make an interface simultaneously usable for users from a diverse range of cultural backgrounds will require a very large amount of adaptation, but the powerful principles of plasticity of user interface design hold the future promise of an optimum tool to achieve cross-cultural usability.},
	journal = {Computer Science Review},
	author = {Miraz, Mahdi H. and Ali, Maaruf and Excell, Peter S.},
	month = may,
	year = {2021},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Adaptive user interface (AUI), Design for All, Dynamic UI Design, Inclusive Design, Interaction design, PEL214, Plasticity, Universal usability, User interface (UI), projeto\_IHC},
	pages = {100363},
}

@inproceedings{DCASE2017,
	title = {{DCASE} 2017 {Challenge} {Setup}: {Tasks}, {Datasets} and {Baseline} {System}},
	url = {https://inria.hal.science/hal-01627981},
	abstract = {DCASE 2017 Challenge consists of four tasks: acoustic scene classification, detection of rare sound events, sound event detection in real-life audio, and large-scale weakly supervised sound event detection for smart cars. This paper presents the setup of these tasks: task definition, dataset, experimental setup, and baseline system results on the development dataset. The baseline systems for all tasks rely on the same implementation using multilayer perceptron and log mel-energies, but differ in the structure of the output layer and the decision making process, as well as the evaluation of system output using task specific metrics.},
	urldate = {2023-05-13},
	booktitle = {Proceedings of the {Detection} and {Classification} of {Acoustic} {Scenes} and {Events} 2017 {Workshop} ({DCASE2017})},
	author = {Mesaros, A and Heittola, T and Diment, A and Elizalde, B and Shah, A and Vincent, E and Raj, B and Virtanen, T},
	month = nov,
	year = {2017},
	keywords = {Acoustic scene classification, Audio tagging, Rare sound events, Sound event detection, Sound scene analysis, classification, dataset, environmental sound},
	pages = {85--92},
}

@book{Luger1997,
	address = {Reading},
	title = {Artificial {Intelligence}: {Structures} and {Strategies} for {Complex} {Problem} {Solving}},
	isbn = {0-8053-1196-3},
	publisher = {Benjamin-Cummings Pub Co},
	author = {Luger, George F. and Stubblefield, William A.},
	year = {1997},
	keywords = {PEL214},
}

@article{Li2020,
	title = {Statistical {Hypothesis} {Testing} versus {Machine} {Learning} {Binary} {Classification}: {Distinctions} and {Guidelines}},
	volume = {1},
	issn = {26663899},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666389920301562},
	doi = {10.1016/j.patter.2020.100115},
	abstract = {Making binary decisions is a common data analytical task in scientific research and industrial applications. In data sciences, there are two related but distinct strategies: hypothesis testing and binary classification. In practice, how to choose between these two strategies can be unclear and rather confusing. Here, we summarize key distinctions between these two strategies in three aspects and list five practical guidelines for data analysts to choose the appropriate strategy for specific analysis needs. We demonstrate the use of those guidelines in a cancer driver gene prediction example. In data science education, two analysis strategies, hypothesis testing and binary classification, are mostly covered in different courses and textbooks. In real data application, it can be puzzling whether a binary decision problem should be formulated as hypothesis testing or binary classification. This article aims to disentangle the puzzle for data science students and researchers by offering practical guidelines for choosing between the two strategies. Hypothesis testing and binary classification are two data analysis strategies taught mostly in different undergraduate classes and rarely compared with each other. As a result, which strategy is more appropriate for a specific real-world data analysis task is often ambiguous. To address this issue, this perspective article clarifies the distinctions between the two strategies and offers practical guidelines to the broad data science discipline.},
	number = {7},
	journal = {Patterns},
	author = {Li, Jingyi Jessica and Tong, Xin},
	month = oct,
	year = {2020},
	note = {arXiv: 2007.01935
Publisher: Cell Press},
	keywords = {DSML 1: Concept: Basic principles of a new data science output observed and reported, PEL307},
	pages = {100115},
}

@article{Hussain2018,
	title = {Model-based adaptive user interface based on context and user experience evaluation},
	volume = {12},
	issn = {1783-7677},
	url = {http://link.springer.com/10.1007/s12193-018-0258-2},
	doi = {10.1007/s12193-018-0258-2},
	abstract = {Personalized services have greater impact on user experience to effect the level of user satisfaction. Many approaches provide personalized services in the form of an adaptive user interface. The focus of these approaches is limited to specific domains rather than a generalized approach applicable to every domain. In this paper, we proposed a domain and device-independent model-based adaptive user interfacing methodology. Unlike state-of-the-art approaches, the proposed methodology is dependent on the evaluation of user context and user experience (UX). The proposed methodology is implemented as an adaptive UI/UX authoring (A-UI/UX-A) tool; a system capable of adapting user interface based on the utilization of contextual factors, such as user disabilities, environmental factors (e.g. light level, noise level, and location) and device use, at runtime using the adaptation rules devised for rendering the adapted interface. To validate effectiveness of the proposed A-UI/UX-A tool and methodology, user-centric and statistical evaluation methods are used. The results show that the proposed methodology outperforms the existing approaches in adapting user interfaces by utilizing the users context and experience.},
	number = {1},
	journal = {Journal on Multimodal User Interfaces},
	author = {Hussain, Jamil and Ul Hassan, Anees and Muhammad Bilal, Hafiz Syed and Ali, Rahman and Afzal, Muhammad and Hussain, Shujaat and Bang, Jaehun and Banos, Oresti and Lee, Sungyoung},
	month = mar,
	year = {2018},
	keywords = {PEL214, projeto\_IHC},
	pages = {1--16},
}

@incollection{Langley1999UserMI,
	address = {Vienna},
	title = {User {Modeling} in {Adaptive} {Interface}},
	url = {http://link.springer.com/10.1007/978-3-7091-2490-1_48},
	abstract = {In this paper we examine the notion of adaptive user interfaces, interactive systems that invoke machine learning to improve their interaction with humans. We review some previous work in this emerging area, ranging from software that filters information to systems that support more complex tasks like scheduling. After this, we describe three ongoing research efforts that extend this framework in new directions. Finally, we review previous work that has addressed similar issues and consider some challenges that are presented by the design of adaptive user interfaces. 1 The Need for Automated User Modeling As computers have become more widespread, the software that runs on them has also become more interactive and responsive. Only a few early users remember the days of programming on punch cards and submitting overnight jobs, and even the era of time-sharing systems and text editors has become a dim memory. Modern operating systems support a wide range of interactive software, from WYSIWYG editors to spreadsheets to computer games, most embedded in some form of graphical user interface. Such packages have become an essential part of business and academic life, with millions of people depending on them to accomplish their daily goals. Naturally, the increased emphasis on interactive software has led to greater interest in the study of human-computer interaction. However, most research in this area has focused on the manner in which computer interfaces present information and choices to the user, and thus tells only part of the story. An equally important issue, yet one that has received much less attention, concerns the content that the interface offers to the user. And a concern with content leads directly to a focus on user models, since it seems likely that people will differ in the content they prefer to encounter during their interactions with computers. Developers of software for the Internet are quite aware of the need for personalized content, and many established portals on the World Wide Web provide simple tools for filtering information. But these tools typically focus on a narrow class of applications and require manual setting of parameters, a process that users are likely to find tedious. Moreover, some facets of users' preferences may be reflected in their behavior but not subject to introspection. Clearly, there is a need for increased personalization in many areas of interactive software, both in supporting a greater variety of tasks and in ways to automate this process. This suggests turning to techniques from machine learning in order to personalize computer interfaces. ? Also affiliated with the Institute for the Study of Learning and Expertise and the Center for the Study of Language and Information at Stanford University.},
	booktitle = {{UM99} {User} {Modeling}},
	publisher = {Springer},
	author = {Langley, Pat},
	editor = {Kay, Judy},
	year = {1999},
	doi = {10.1007/978-3-7091-2490-1_48},
	keywords = {PEL214, projeto\_IHC},
	pages = {357--370},
}

@article{GarciaCeja2019,
	title = {User-adaptive models for activity and emotion recognition using deep transfer learning and data augmentation},
	volume = {30},
	issn = {0924-1868},
	url = {http://link.springer.com/10.1007/s11257-019-09248-1},
	doi = {10.1007/s11257-019-09248-1},
	number = {3},
	journal = {User Modeling and User-Adapted Interaction},
	author = {Garcia-Ceja, Enrique and Riegler, Michael and Kvernberg, Anders K. and Torresen, Jim},
	month = jul,
	year = {2020},
	keywords = {PEL214, projeto\_IHC},
	pages = {365--393},
}

@incollection{Henricksen2002,
	title = {Modeling {Context} {Information} in {Pervasive} {Computing} {Systems}},
	volume = {2414},
	url = {http://link.springer.com/10.1007/3-540-45866-2_14},
	abstract = {As computing becomes more pervasive, the nature of applications must change accordingly. In particular, applications must become more flexible in order to respond to highly dynamic computing environments , and more autonomous, to reflect the growing ratio of applications to users and the corresponding decline in the attention a user can devote to each. That is, applications must become more context-aware. To facilitate the programming of such applications, infrastructure is required to gather, manage, and disseminate context information to applications. This paper is concerned with the development of appropriate context modeling concepts for pervasive computing, which can form the basis for such a context management infrastructure. This model overcomes problems associated with previous context models, including their lack of formality and generality, and also tackles issues such as wide variations in information quality, the existence of complex relationships amongst context information and temporal aspects of context. 1 Motivation The emergence of new types of mobile and embedded computing devices and developments in wireless networking are driving a spread in the domain of computing from the workplace and home office to other facets of everyday life. This trend will lead to the scenario, often termed pervasive computing, in which cheap, interconnected computing devices are ubiquitous and capable of supporting users in a range of tasks. It is now widely acknowledged that the success of pervasive computing technologies will require a radical design shift, and that it is not sufficient to simply extrapolate from existing desktop computing technologies [1, 2]. In particular, pervasive computing demands applications that are capable of operating in highly dynamic environments and of placing fewer demands on user attention. In order to meet these requirements, pervasive computing applications will need to be sensitive to context. By context, we refer to the circumstances or situation in which a computing task takes place.},
	urldate = {2022-10-26},
	booktitle = {{LNCS}},
	publisher = {Springer-Verlag},
	author = {Henricksen, Karen and Indulska, Jadwiga and Rakotonirainy, Andry},
	year = {2002},
	doi = {10.1007/3-540-45866-2_14},
	keywords = {PEL214},
	pages = {167--180},
}

@inproceedings{Du2019,
	address = {Neuburg an der Donau, Germany},
	title = {Automotive microphone performance: {From} specification to user experience},
	volume = {2019-September},
	url = {http://www.aes.org/e-lib/browse.cfm?elib=20531},
	urldate = {2023-05-27},
	booktitle = {Procedings [...]},
	publisher = {Audio Engineering Society},
	author = {Du, Yu and Dobos, Viktor and Varga, Bálazs and Yang, Ruiting},
	year = {2019},
	note = {tex.venue: Neuburg an der Donau, Germany
tex.eventyear: 2019},
	pages = {p. (paper 5)},
}

@inproceedings{Belo2022,
	address = {New York, NY, USA},
	title = {{AUIT} – the {Adaptive} {User} {Interfaces} {Toolkit} for {Designing} {XR} {Applications}},
	isbn = {978-1-4503-9320-1},
	url = {https://dl.acm.org/doi/10.1145/3526113.3545651},
	doi = {10.1145/3526113.3545651},
	language = {English},
	booktitle = {The 35th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Evangelista Belo, João Marcelo and Lystbæk, Mathias N. and Feit, Anna Maria and Pfeuffer, Ken and Kán, Peter and Oulasvirta, Antti and Grønbæk, Kaj},
	month = oct,
	year = {2022},
	note = {Series Title: UIST 2022 - Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology; },
	keywords = {PEL214, adaptive user interfaces, context-awareness, extended reality, multi-objective optimization, toolkit},
	pages = {1--16},
}

@article{Chin2001,
	title = {Empirical {Evaluation} of {User} {Models} and {User}-{Adapted} {Systems}},
	volume = {11},
	issn = {1573-1391},
	url = {https://doi.org/10.1023/A:1011127315884},
	doi = {10.1023/A:1011127315884},
	abstract = {Empirical evaluations are needed to determine which users are helped or hindered by user-adapted interaction in user modeling systems. A review of past UMUAI articles reveals insufficient empirical evaluations, but an encouraging upward trend. Rules of thumb for experimental design, useful tests for covariates, and common threats to experimental validity are presented. Reporting standards including effect size and power are proposed.},
	number = {1},
	journal = {User Modeling and User-Adapted Interaction},
	author = {Chin, David N},
	year = {2001},
	keywords = {PEL214},
	pages = {181--194},
}

@article{Aubauer2001,
	title = {Optimized second-order gradient microphone for hands-free speech recordings in cars},
	volume = {34},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639300000431},
	doi = {10.1016/S0167-6393(00)00043-1},
	number = {1-2},
	urldate = {2023-05-06},
	journal = {Speech Communication},
	author = {Aubauer, Roland and Leckschat, Dieter},
	month = apr,
	year = {2001},
	pages = {13--23},
}
